{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYlBedDIyVjj",
    "outputId": "63406db7-0bd5-479e-b884-c2b8e1a34b49"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySzbtyHm4TP5"
   },
   "source": [
    "## imports + méthodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoDurrWzxXny"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_confusion_matrix(confusionmatrix, classes):\n",
    "    sns.set(color_codes=True)\n",
    "    plt.figure(1, figsize=(8, 5))\n",
    "\n",
    "    plt.title(\"Matrice de confusion\")\n",
    "\n",
    "    sns.set(font_scale=1.4)\n",
    "    ax = sns.heatmap(confusionmatrix, annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Scale'},fmt='g')\n",
    "\n",
    "    ax.set_xticklabels(classes,rotation=45)\n",
    "    ax.set_yticklabels(classes,rotation=0)\n",
    "\n",
    "    ax.set(ylabel=\"True Label\", xlabel=\"Predicted Label\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def MyshowAllScores(y_test, y_pred):\n",
    "    classes = ['non_hope', 'neutral', 'hope']\n",
    "    label_map = {0: 'non_hope', 1: 'hope', 2: 'neutral'}\n",
    "    y_test_labels = [label_map[label] for label in y_test]\n",
    "    y_pred_labels = [label_map[label] for label in y_pred]\n",
    "    print(\"Accuracy : %0.3f\" % (accuracy_score(y_test, y_pred)))\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test_labels, y_pred_labels, digits=5))\n",
    "    cnf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "    # Afficher la matrice de confusion avec les étiquettes personnalisées\n",
    "    plot_confusion_matrix(cnf_matrix, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-fb87mPrMEJ"
   },
   "source": [
    "# tweet collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9ahOmVy4eGh"
   },
   "source": [
    "## Création des dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSeiunoHkuU-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtkcVPEj8l7X"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([dfAvenirEspoir, dfCapacite, dfChine, dfFestival, dfFete, dfService, dfSecurite, dfShow, df1, df2, df3, df4])\n",
    "df['labelAsma'] = df['labelAsma'].str.strip()\n",
    "df['labelGPT'] = df['labelGPT'].str.strip()\n",
    "df = df.dropna(subset=['labelAsma'])\n",
    "df = df.drop_duplicates()\n",
    "df['labelAsma'] = df['labelAsma'].replace({'hope': 'hope', 'no hope': 'non_hope'})\n",
    "df['labelGPT'] = df['labelGPT'].replace({'hope': 'hope', 'no hope': 'non_hope'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2kYiD_e5U7P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCxujl6V4tzW"
   },
   "source": [
    "## Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCWpoQ9G5FNj",
    "outputId": "5b0c407f-8a72-4779-b445-ff949adecb39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My data frame size : 325\n"
     ]
    }
   ],
   "source": [
    "print(\"My data frame size :\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3h8vPFbwpUH",
    "outputId": "f5c906a1-bc41-4578-822b-b9df0861c2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hope' 'non_hope' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "print(df['labelAsma'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qYSfcwHi_PJ",
    "outputId": "b5b13426-1744-495f-a0dd-4429ac79261f",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('labelAsma').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qh20phoPFonr",
    "outputId": "29b76e7f-de2f-4a5f-9a5c-10d51ee2b06a",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supposons que vous avez un DataFrame appelé df contenant les données des tweets\n",
    "\n",
    "# Filtrer les tweets étiquetés avec \"hope\" et \"no hope\" pour les étiquettes \"labelAsma\"\n",
    "filtered_asma_hope = df[(df['labelAsma'] == 'hope')]\n",
    "filtered_asma_no_hope = df[(df['labelAsma'] == 'non_hope')]\n",
    "filtered_asma_neutral = df[(df['labelAsma'] == 'neutral')]\n",
    "\n",
    "# Obtenir les nombres de tweets pour chaque catégorie\n",
    "count_asma_hope = len(filtered_asma_hope)\n",
    "count_asma_no_hope = len(filtered_asma_no_hope)\n",
    "count_asma_neutral = len(filtered_asma_neutral)\n",
    "\n",
    "\n",
    "# Créer les données pour l'histogramme\n",
    "labels = ['labelAsma - Hope', 'labelAsma - non_hope','labelAsma - neutral']\n",
    "counts = [count_asma_hope, count_asma_no_hope, count_asma_neutral]\n",
    "\n",
    "# Créer l'histogramme\n",
    "plt.bar(labels, counts)\n",
    "plt.xlabel('Catégories')\n",
    "plt.ylabel('Nombre de tweets')\n",
    "plt.title('Nombre de tweets étiquetés en \"hope\" et \"non_hope\"')\n",
    "\n",
    "# Afficher l'histogramme\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90C2lMR1PT_T",
    "outputId": "e72539c3-9273-43b5-bf03-3d6e5b058bb1",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supposons que vous avez un DataFrame appelé df contenant les données des tweets\n",
    "\n",
    "\n",
    "# Filtrer les tweets étiquetés avec \"hope\" et \"no hope\" pour les étiquettes \"labelGPT\"\n",
    "filtered_gpt_hope = df[(df['labelGPT'] == 'hope')]\n",
    "filtered_gpt_no_hope = df[(df['labelGPT'] == 'non_hope')]\n",
    "filtered_gpt_neutral = df[(df['labelGPT'] == 'neutral')]\n",
    "\n",
    "# Obtenir les nombres de tweets pour chaque catégorie\n",
    "count_gpt_hope = len(filtered_gpt_hope)\n",
    "count_gpt_no_hope = len(filtered_gpt_no_hope)\n",
    "count_gpt_neutral = len(filtered_gpt_neutral)\n",
    "\n",
    "# Créer les données pour l'histogramme\n",
    "labels = ['labelGPT - Hope', 'labelGPT - non_hope','labelGPT - neutral']\n",
    "counts = [count_gpt_hope, count_gpt_no_hope, count_gpt_neutral]\n",
    "\n",
    "# Créer l'histogramme\n",
    "plt.bar(labels, counts)\n",
    "plt.xlabel('Catégories')\n",
    "plt.ylabel('Nombre de tweets')\n",
    "plt.title('Nombre de tweets étiquetés en \"hope\" et \"non_hope\" et \"neutral\"')\n",
    "\n",
    "# Afficher l'histogramme\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZH05HYQt4xgn"
   },
   "source": [
    "# gpt model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2ozz8qfSRvI",
    "outputId": "02d4f52c-99cf-49af-936c-a0fb16462636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1 0 2]\n",
      "Accuracy : 0.748\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hope    0.67778   0.89051   0.76972       137\n",
      "     neutral    0.46667   0.29167   0.35897        24\n",
      "    non_hope    0.88525   0.69231   0.77698       156\n",
      "\n",
      "    accuracy                        0.74763       317\n",
      "   macro avg    0.67656   0.62483   0.63522       317\n",
      "weighted avg    0.76389   0.74763   0.74219       317\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvgAAAIQCAYAAAD0NRoMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACDv0lEQVR4nOzdd3gU5dfG8e+mkoRACBB6x4QWOig19I4gShEFEZCmgDSlKAIioIAFAZGoVEFFBRQRkGoBkSK9SieQQAgJ6XXfP3izP5YklLDJZpf7c117mZ15ZuZMEsmZs2eeMRiNRiMiIiIiImIXHKwdgIiIiIiIWI4SfBERERERO6IEX0RERETEjijBFxERERGxI0rwRURERETsiBJ8ERERERE7ogRfRERERMSOKMEXEREREbEjSvBFREREROyIEnwReex9+umn+Pn5WTsMAH788Uf8/Py4fPmytUN5ZL///judOnXC398fPz8/bt26ZdH929P3SkTEkpTgi0i2SU3I/Pz82Lt3b5r1RqORgIAA/Pz8GDhwYKaOsWDBAjZv3vyoocojunnzJq+//jq5cuVi4sSJfPDBB7i5uVk7LBGRx4ISfBHJdq6urqxbty7N8n/++Yfg4GBcXFwyve/PP//8oRP8wYMHc+jQoUwfU9I6fPgw0dHRDB8+nK5du9KpUyecnZ0teoxOnTpx6NAhihUrZtH9iojYOiX4IpLtAgIC2LBhA0lJSWbL161bR+XKlSlYsGC2xBETEwOAk5MTrq6u2XLMx0VYWBgAnp6eWXYMR0dHXF1dMRgMWXYMERFbpARfRLJd+/btCQ8P56+//jItS0hIYOPGjXTs2DHdbb788kt69OjBk08+SdWqVenSpQsbNmwwG+Pn50dMTAyrV682tQKNHTsW+F+f/X///ceoUaOoU6cOPXv2NFt3t7Vr1/Lcc89RrVo16tSpwwsvvMCff/5pNmbHjh307NmT6tWrU6NGDQYMGMDp06cf6Ptw+vRpevfuTdWqVWncuDHz588nJSUl3bGPcpxbt24xbdo0mjVrRpUqVWjcuDFvvPGGKQkHuHHjBuPHj6d+/fr4+/vz9NNPs3r1arP9XL58GT8/P7788ku+/fZbWrRoQZUqVXj22WfNPgHp1asXb775JgDPPfec2c+hWbNmpq/v1KtXL3r16mW2bNmyZbRv3970/e/SpQs///yzaX1GPfhff/017du3p0qVKjRs2JDJkyen6f/v1asXHTp04L///qNXr15Uq1aNRo0aERgY+EDfUxGRnMzJ2gGIyOOnWLFiVK9enV9++YWAgADg9g2ZkZGRtGvXjmXLlqXZZunSpTRr1oyOHTuSmJjIL7/8wvDhw/n8889p0qQJAB988AFvvfUWVatWpVu3bgCULFnSbD/Dhw+nVKlSjBgxAqPRmGGMc+fO5dNPP6VGjRoMGzYMZ2dnDh48yN9//03Dhg0BWLNmDWPHjqVhw4aMHj2a2NhYVq5cSc+ePVm9ejXFixfPcP/Xr1+nd+/eJCcnM2DAANzc3Pjuu+/S/SThUY4THR3NCy+8wJkzZ3j22WepVKkSN2/eZOvWrYSEhODt7U1cXBy9evXi4sWLvPDCCxQvXpwNGzYwduxYbt26xUsvvWS2z3Xr1hEdHU337t0xGAx88cUXDB06lM2bN+Ps7MygQYMoU6YM3377LcOGDaN48eJpfg7389133zF16lRat25N7969iY+P5+TJkxw8eDDDi0C4fbE2d+5c6tevz/PPP8+5c+dYuXIlhw8fZuXKlWZtQhEREfTv35+WLVvStm1bNm7cyKxZs/D19TX9XoqI2CSjiEg2+eGHH4y+vr7GQ4cOGZcvX26sUaOGMTY21mg0Go3Dhg0z9urVy2g0Go1NmzY1DhgwwGzb1HGpEhISjB06dDD27t3bbHn16tWNb775Zppjz5kzx+jr62scOXJkhutSnT9/3lihQgXjq6++akxOTjYbm5KSYjQajcaoqChj7dq1jW+99ZbZ+uvXrxtr1aqVZvnd3nvvPaOvr6/x4MGDpmU3btww1qpVy+jr62u8dOmSRY7zySefGH19fY2bNm1Ksy71XBYvXmz09fU1rl271rQuISHB2L17d2P16tWNkZGRRqPRaLx06ZLR19fXWLduXWN4eLhp7ObNm42+vr7GrVu3mpbd+bO+U9OmTdP9+bz44ovGF1980fR+8ODBxvbt29/z3FKPkfq9unHjhrFy5crGvn37mv3cli9fbvT19TV+//33Zsfz9fU1rl692rQsPj7e2KBBA+PQoUPveVwRkZxOLToiYhVt27YlPj6ebdu2ERUVxfbt2+9Zmc2VK5fp64iICCIjI6lVqxbHjh17qOP26NHjvmM2b95MSkoKr776Kg4O5v9MpvZ779y5k1u3btG+fXvCwsJMLwcHB6pVq8bu3bvveYwdO3ZQvXp1qlatalrm7e2d5nvwqMfZtGkTFSpUoGXLlmnWpZ7L77//TsGCBenQoYNpnbOzM7169SImJoY9e/aYbdeuXTvy5s1rel+7dm0ALl26dM9YHkaePHkIDg5+qJufd+7cSWJiIr179zb7uXXt2pXcuXOzY8cOs/Hu7u506tTJ9N7FxQV/f3+LnoeIiDWoRUdErMLb25t69eqxbt064uLiSE5OpnXr1hmO37ZtG5999hnHjx8nISHBtPxhb7C8VztLqosXL+Lg4EC5cuUyHHP+/HmANO0rqXLnzn3PY1y5coVq1aqlWV6mTBmLHufixYu0atXqnmOCgoIoVapUmouZ1PO/cuWK2fIiRYqYvU9N9i05z/0rr7zCzp076dq1K6VKlaJBgwZ06NCBWrVqZbhNapxly5Y1W+7i4kKJEiUICgoyW164cOE0vz958+bl5MmTFjoLERHrUIIvIlbToUMH3n77bUJDQ2ncuDF58uRJd9zevXsZPHgwderU4Z133qFgwYI4Ozvzww8/pDvd5r1YarYc4//373/wwQfpzvrj6OhoU8d5GBkd03iPexruJzk52Wy/5cqVY8OGDWzfvp0//viDTZs2sWLFCl599VWGDRuW6ePcyRrfOxGR7KAEX0SspmXLlrzzzjscOHCAjz76KMNxGzduxNXVlS+//NJsjvwffvghS+IqWbIkKSkpnDlzhooVK6Y7pkSJEgDkz5+f+vXrP/QxihYtyoULF9IsP3funEWPU7JkyfvOtlOsWDFOnjxJSkqKWRX/7NmzplgtJW/evOlW+q9cuWI611Tu7u60a9eOdu3akZCQwNChQ1mwYAEDBw5M90ItNc6zZ8+a7SshIYHLly9n6vsnImKL1IMvIlbj4eHBpEmTGDp0KM2aNctwnKOjIwaDgeTkZNOyy5cvs2XLljRj3d3dH7lVpEWLFjg4ODBv3rw001amVqkbNWpE7ty5+fzzz0lMTEyzjzunoExPQEAABw4cMOsxDwsLM5sG0hLHadWqFSdOnOC3335Lsy71XBo3bsz169dZv369aV1SUhLLli3D3d2dOnXq3PMYD6NEiRIcPHjQrM1q27ZtXL161WzczZs3zd67uLhQrlw5jEZjut8HgPr16+Ps7MyyZcvMPk34/vvviYyM1Mw4IvLYUAVfRKzqmWeeue+YgIAAFi1aRP/+/enQoQM3btxgxYoVlCxZMk2/dOXKldm1axeLFi3Cx8eH4sWLp9vrfi+lSpVi0KBBzJ8/n549e9KqVStcXFw4fPgwPj4+jBo1ity5czNp0iTeeOMNunTpQrt27fD29ubKlSvs2LGDmjVrMnHixAyP0b9/f9auXUv//v3p3bu3aZrMokWLmp3Tox6nX79+bNy4keHDh/Pss89SuXJlIiIi2Lp1K5MnT6ZChQp0796db7/9lrFjx3L06FGKFSvGxo0b2b9/P+PHj79vn//D6Nq1Kxs3bqR///60bduWixcv8vPPP6eZRrNfv34UKFCAmjVrkj9/fs6ePcvy5csJCAjIMB5vb28GDhzI3Llz6d+/P82aNePcuXOsWLHCNLe/iMjjQAm+iOR49erV47333iMwMJBp06ZRvHhxRo8eTVBQUJoEf+zYsUycOJGPP/6YuLg4nnnmmYdO8OH2fPnFixdn+fLlfPTRR7i5ueHn52c260rHjh3x8fFh4cKFfPnllyQkJFCoUCFq165Nly5d7rl/Hx8fli5dytSpU1m4cCFeXl706NEDHx8fJkyYYDb2UY7j4eHB119/zaeffspvv/3G6tWryZ8/P/Xq1aNQoULA7RmKli1bxqxZs1i9ejVRUVGUKVOG6dOn33f/D6tRo0aMHTuWRYsWMW3aNKpUqcKCBQt4//33zcZ1796dn3/+mUWLFhETE0PhwoXp1asXQ4YMuef+hw4dire3N8uXL2f69OnkzZuXbt26MXLkSLM58EVE7JnB+Ch3RYmIiIiISI6iHnwRERERETuiBF9ERERExI4owRcRERERsSNK8EVERERE7IgSfBERERERO6IEX0RERETEjmgefBERERHJsY4ePcrOnTs5fPgwR44cISgoCIAtW7ZQvHjxNONDQ0PZvn07O3bs4PDhw4SGhuLi4sITTzxBx44d6dGjB05OGafA69evZ9myZabnrPj5+dG7d2/atm2bNSeYBTQP/mPAreTz1g5BJI2Yixk/fVXEGqITQ6wdgoiZ3M5NrHZsS+cOsRdXZnrbIUOGsGXLljTLM0rwR48ezc8//4yjoyOVKlWiRIkShIaGcuDAARISEqhTpw6BgYG4ubml2fajjz5iwYIFuLi40KBBAwD++usvEhISGDJkCMOHD8/0eWQnVfBFREREJMeqXr06vr6+VKlSBX9/f7p06UJoaGiG4728vBg+fDhdu3alYMGCpuXnzp2jb9++7NmzhwULFjBixAiz7fbu3cuCBQvIkycP33zzDeXKlQPgzJkz9OjRg/nz59O4cWNq1KiRNSdqQerBFxEREREzBoODRV+PYsCAAbz++uu0aNGCQoUK3Xf8W2+9xZAhQ8ySe4AyZcowatQoAH7++ec0233xxRcADBo0yJTcA5QrV46BAweajcnplOCLiIiIiBkDDhZ95RQVKlQA4Nq1a2bL4+Pj2blzJ0C6vfbt2rUD4M8//yQhISGLo3x0Oec7LiIiIiKShS5cuACQprp/7tw54uPjyZcvH0WLFk2zXdGiRfHy8iIuLo5z585lS6yPQgm+iIiIiJjJSS06lrR48WIAmjdvbrY8dWaewoULZ7ht6rorV65kTXAWpJtsRURERMSMpZPyuxPqu6U3S46lLV26lH/++QcvLy9TT32qmJgYgHRn1knl7u4OQHR0dNYFaSE555JKRERERCQL/PXXX7z//vs4ODgwffr0NC069kYVfBERERExYzAYLLq/7KjQZ+TQoUO89tprJCUlMXXqVJo1a5ZmTGp1PjY2NsP9pFb5PTw8siZQC1KCLyIiIiJ3sY8mj1OnTvHKK68QExPDm2++SdeuXdMdV6xYMQCCg4Mz3FfquvRuws1p7OOnJyIiIiJyhwsXLtC3b1/Cw8N59dVX6du3b4Zjy5Qpg6urKzdv3kz3JtorV64QHh5Orly5KFOmTFaGbRFK8EVERETEjK3PonP16lX69OnD9evX6dOnD8OGDbvneFdXV+rXrw/Ar7/+mmb9+vXrAWjYsCEuLi6WD9jClOCLiIiIiBlbTvDDwsJ4+eWXuXLlCt27d2fcuHEPtF3//v0B+Pzzzzlz5oxp+ZkzZ/j888/NxuR06sEXERERkRxr+/btzJ8/3/Q+IiICgNdee81UTQ8ICODVV18F4O233+bcuXO4uLgQHx/P2LFj093vG2+8gbe3t+l97dq1GThwIJ9//jnPPPOMqaK/c+dO4uPjGTJkCDVq1MiSc7Q0JfgiIiIiYsaQg5o8wsLCOHjwYJrlx48fN31dtmxZ09e3bt0CICEhgTVr1mS439dee80swQcYOXIkFSpUYOnSpezevRuASpUq8dJLL9G2bdtHOY1sZTAajUZrByFZy63k89YOQSSNmIsTrR2CiJnoxBBrhyBiJrdzE6sd2/uJ1yy6v7DTcy26P7m3nHN5JiIiIiIij0wtOiIiIiJixhoz34jlKMEXERERETNK8G2bfnoiIiIiInZEFXwRERERMWPAYO0Q5BEowRcRERERM2rRsW366YmIiIiI2BFV8EVERETEjCr4tk0JvoiIiIiYUYJv2/TTExERERGxI6rgi4iIiMhdVAO2ZUrwRURERMSMWnRsm356IiIiIiJ2RBV8ERERETGjCr5tU4IvIiIiImYMavKwafrpiYiIiIjYEVXwRURERMSMWnRsmxJ8ERERETFjMBisHYI8Al2eiYiIiIjYEVXwRURERMSMWnRsmxJ8ERERETGjWXRsm356IiIiIiJ2RBV8ERERETGjFh3bpgRfRERERMwowbdt+umJiIiIiNgRVfBFRERExIxusrVtSvBFRERExJxadGyafnoiIiIiInZEFXwRERERMaObbG2bEnwRERERMWMwGKwdgjwCXZ6JiIiIiNgRVfBFRERExIxm0bFtSvBFRERExIx68G2bfnoiIiIiInZEFXwRERERMaebbG2aEnwRERERMaceD5umH5+IiIiIiB1RBV9EREREzKlFx6YpwRe7VqhgXpo29KdW1bLUrFqGapVL4+GeiwuXrlOhwbAMt6tbozztWtSkfp0K+JUvSr68HkTFxHH81GXWrP+HwOWbiYtPtPi2Ivezfftefv99P0ePniH4aig3b97C0dGBwoUL8NRT/vR+qSNlyhSzdphiR0JDI/hn13GOHb3AsaMXOHniEnGxCRQpmp91m6alu43RaOTIoXP8vv0QB/79j3Nng7l1Kxp391yULVeE5i1r8my3xuTK5ZLNZyMPTAm+TVOCL3at69P1mflO74fapkmDyvy68i3T+/MXr3EpKJRiRfJTv04F6tepQN+ezenwwjSCgsMstq3Ig1iy+Cd27TqEk5MjBQvmw9e3FLduRXHpUjDnzgXx/febmfH+cNq3b2TtUMVObPp1D7PfX/VQ2+zZfYLB/T82vS9aLD9FingTEnyTg/+e4eC/Z/hx1R/MD3ydQoXzWThiEVGCL3btVmQsW/88zP5DZ9l/6Cwlihbg/Ym97rmNwWDgwqXrzF+0ge9+2knwtXDTunYtahL44WAqPFGMZfOH0azLJIttK/IgOnduSv9XulCrVkVy5XI1LQ8JucHUdwP57be/mTD+U2rVqkjhwgWsGKnYCw8PN+o+VYGKlUtRqXIpgq+G8dHM7++5jdEIRYrmp8cLTWndri4FC+Y1rft9+0HeGb+Y8+eCGTc6kK+Wv5HVpyCZobs0bZrBaDQarR1ETvTjjz8ybtw4du3ahbe3t7XDeSRuJZ+3dgg5RteO9Vg6b9g9W3Q8c7sRG5dAUlJyuut7dG7AojmvAVCn1ZscOXHRIts+bmIuTrR2CHYnPj6BRg1f5tataCZNGkSP59tYOySbEp0YYu0QbMLG9XsY/8YX92zRiYqKxdXVBWdnx3TXr1+3m7fHfgXANz+8zRN+xbMsXluW27mJ1Y79RKPPLbq/038MtOj+5N50fSZyl8io2AwTdIAN2w6Yvq7wRFGLbSvyqFxdXSheohAAMbFxVo5GHme5c7tlmNwDNGxUxfT1ubNXsyMkkceKWnREHpLbHTeFRcfEZ9u2IvdzM+wW584GAeDv/4SVoxHJWPwdEw3kcnO9x0ixGt1ja9NyVAV/7NixdOjQgd27d9O5c2eqV6/Oc889x5EjR0xj4uPjmT59Og0bNsTf359OnTrx22+/PfR+HlRwcDD9+/enevXqtGrVijVr1qQZ880339C6dWuqVKlCs2bNmD9/PikpKab1P/74I35+fhw4cIDevXtTrVo1mjVrxvffp+1h/Pfff+nduzfVq1enVq1ajBo1ihs3bjx03JJ1uneqD0BCQhJ/7z2VbduKZCQsLILfd+yjX79JxMbG06FDY+rUqWztsEQy9Osv/wDg5ORItRrlrByNpMvBYNmXZKscleADXL9+nalTp9KvXz8+/vhj4uPjee2110hMvH21P3r0aL799lv69+/PvHnzKF++PEOHDmXLli0PtZ8HNXr0aBo2bMi8efOoWLEiY8eO5cyZM6b1y5Yt45133qFRo0YsWLCAZ555hrlz5zJz5sw0+xo5ciQNGjRg7ty5PPnkk0yYMIHff//dtP7ff/+lV69eeHp68tFHH/Huu+9y+PBhhgwZ8lAxS9YpWbwAY4d1AeCLrzdzMyI6W7YVudvmzX9Twa8zFfw6U7/eSwwY8C63bkUzecpgZs4aYe3wRDJ0JSiULz7/BYBnuzUmb14PK0ckYn9yXItOREQEy5cv54knbn+87ObmRu/evTl48CC5c+dm06ZNTJ48mR49egDQuHFjgoKCmDdvHs2bN3+g/dSuXfuB43nhhRd44YUXAKhRowY7duxg48aNDBkyhOTkZObNm0f79u15663bUyM2bNiQxMREvvrqKwYMGEC+fP+b/qtTp04MHHj7JpNGjRpx6dIl5s2bR+PGjQGYPXs2VapUYe7cuRj+f/5ZX19fOnTowI4dOwgICMjU91Qsw8Pdle8CR5E3jzsn/wvi7RnfZMu2Iunx8vKkZs2KpBhTuHYtjJDgGwQFXWPdut+pU7syZcvppkXJeWJi4hg17DOio+IoVaYQQ19/xtohSUY0D75Ny3EVfB8fH1NSDlC+fHkAQkJC2LdvHwBt2pjPDNG2bVuOHTtGTEzMA+3nYTRs2ND0tbu7O0WLFiU4OBiAs2fPcvPmzTTxtGvXjsTERA4dOmS2vGXLlmbvW7VqxdGjR0lOTiY2Npb9+/fTpk0bkpOTSUpKIikpidKlS1OkSBEOHz78UHGLZeVydeaHr8ZQrXJprobcpMvLM4mJfbAe+kfZViQjtWtXZsXK6Xzzzfts3RrItu1f0KVLM/b8c5Ru3d4gKOiatUMUMRMXl8Drr87j1MnLFCiYl0/mvYabu/rvcyyDhV+P4OjRowQGBjJs2DCaNWuGn58ffn5+XL58+Z7bXbx4kTFjxpjaulu2bMmsWbOIjs74E3Sj0cjKlSvp0qUL1atXp27duvTt25ddu3Y92klksxxXwc+TJ4/Ze2dnZ+B2731ERATOzs54eXmZjSlQoABGo5HIyEjc3d3vu5+H4enpmWY/CQkJwO1PCQDy589vNib1fer6u5ffGXdiYiI3b94kOTmZ5ORkpk+fzvTp09PEcfWqZhmwFhcXJ777YhQB9SsTcj2CNj2mcvbCg10oPsq2Ig/Dx8eb96YNJTjkBn/9eYAFC1bx7ruvWjssEQASEhIZNewz9u05hbe3Jwu+HEmJkj7WDktsxLx589K0Yt/P0aNH6dWrF9HR0VSuXJnatWtz6NAhAgMD2bFjBytWrEiT4xmNRsaMGcPPP/+Mh4cHjRo1Ijo6mr///pudO3fy7rvv0rVrV0ueWpbJcQn+veTNm5fExEQiIiLIm/d/D80IDQ3FYDCk+UFltdQLjbAw8yeSpt4Ue2eMqcsLFSpkeh8aGoqzszP58uUjPj4eg8HAwIEDadGiRZpj3dnqI9nH2dmRbz4fScuAalwLjaDt81M5deZKlm8rkllNm9bhrz8PcPTImfsPFskGiYlJjB6+gL93HiOftycLvhpJmbKFrR2W3E8OujG2evXq+Pr6UqVKFfz9/enSpQuhoaEZjk9OTmbkyJFER0czatQoBgwYAEBCQgLDhg1j27ZtzJw5kylTpphtt3btWn7++WeKFy/OihUrTDnbnj17ePnll5k8eTL169enWLFiWXeyFpLjWnTupVatWgBs2LDBbPmGDRuoVKmSqXqfXcqUKYO3t3eaeH799VecnZ2pWrWq2fK7Z/vZtGkTlStXxtHREXd3d6pXr87Zs2fx9/dP8ypeXP202c3JyZGvP3udts1rEBoWSfue0zh+6t4fB1piW5FHkfz/z2FITk65z0iRrJeYmMybIxfy1x9HyOvlwWeBr1OuvJ4BYhMMBsu+HsGAAQN4/fXXadGihVmhNCNbtmzh/Pnz+Pr68sorr5iWu7i4MGXKFJycnPjhhx+4efOm2XZffvklAGPGjDE7Tp06dejatSuJiYksWbLkkc4lu9hUBb9ChQq0atWKGTNmEBcXR5kyZfjpp5/4999/mT9/frbH4+joyJAhQ5g6dSre3t4EBARw4MABAgMDeemll9JU3deuXUuuXLmoVKkS69evZ8+ePSxcuNC0/o033uCll17i9ddfp3379uTJk4fg4GB27txJly5dePLJJ7P7FB9bjo4OLJ83jI6tahMaFkm756c+8FNnH2VbkUe1adPtPtGKlcpYORJ53CUlJTNu9EJ2bDtIXi8PFnwxQk+slWyxbds2AFq3bm2atCSVj48PtWrVYvfu3ezYsYPOnTsDcPnyZU6dOoWrqyvNmjVLs8927dqxYsUKtmzZwvjx47P8HB6VTSX4ADNnzuTDDz8kMDCQ8PBwypYty5w5c9L9YWSHXr164eTkxOLFi1m5ciUFCxbktddeY9CgQWnGzp49mw8//JB58+aRP39+3n33XbOZcWrWrMmKFSv49NNPGTduHImJiRQuXJinnnqKUqVKZedpPdYcHAx89fGrdGpblxs3I2nf8z0OH3+wBP1RthW5n8OH/2PL5t083akJZcuaf0R85cp1Zs9eyr59x3F0dKB3745WilLk9idIE8ctYtuWA+TN68FnX4zAt0IJa4clDyPndOg8tOPHjwNQpUqVdNdXrlyZ3bt3c+LECdOy1K+feOIJXFxc0mxTqVIl4PaFQFRUFLlz57Z02BaVoxL8GTNmpFmWJ08eTp48aXqfK1cuxo8ff8+rpwfZz/106dKFLl26pFm+du3aNMuef/55nn/++fvus1SpUixbtuyeY/z9/c2q+vJoihfxZtev//t9cPn/R6cXL5qfSwf+933etfck3frPBuC5DvXo9v8PpIqJieejd/tkuP8l3+5g6XfbTe8fZVuR+4mJiWXBglUsWLAKLy9PihYtiLOzEzduRBAUdA2j0Yi7ey6mvvcalSqVtXa4YieCr4bRs+tU0/ukxNttYCHBYTRrONK0vHqN8nz46e3ntvy2cS8bf90DQC43F95/b2WG++/0TAM6dWmQFaHLo8hBPfgP68qV2/e7FS6c/r0eqe03qeMeZBsPDw88PT2JjIzkypUr+Pr6WjJki8tRCb6IpTk4OlDAO+3N1453Lc/r+b/7N1xc/ve/RYliBShRrECG+9/6p/nTkR9lW5H7qVChDG+9/Qr//HOEUycvcOlSMLGx8XjkdqdqNV/q16tK9x6tKVw44987kYeVkpJCRHjaaQVTUoxmy6MiY01fJyQkmb4OCb5JSLB5r/OdnnyqooUilZzszmcVpedhZ8m5l9Rp093c3NJd7+Fx++Fqd06Xeb9t4PZ06ZGRkfecZjOneCwT/JSUFFJSMr4BzdHRMU3Pltimi5dDcSt5/09X7rT8+99Z/v3v9x9o4W1F7idv3ty8+GJ7XnyxvbVDkcdI0WIF2Hfk84fa5unO9Xm6c/0sikiyhdIgm/ZYJvjz5s1j7ty5Ga6fPn16uu05mZVRu4+IiIhITmS0cKHTkhX6+3F3dyciIoLY2Nh016dW4FMr+anbABluA/+r8t+5XU71WCb43bp1o0mTJhmu15SUIiIiIrapaNGiREREEBwcTIUKFdKsDwkJMY27cxuA4ODgdPcZHR1NZGRkmu1yqscywS9UqNADzaMqIiIi8liy4ZtsK1asyPHjxzly5Ei6Bd2jR48CmCX/qV+fPn2ahISENDPpHDt2DLhdBM7pM+iAjT3oSkRERESygcHCr2zUtGlTADZu3IjRaDRbd+3aNfbt24eTkxONGzc2LS9evDi+vr7Ex8ezdevWNPtcv349cP+bhXMKJfgiIiIiYjeaNWtG6dKlOXXqFIGBgablCQkJTJw4kaSkJJ599lm8vb3NtuvXrx9w+5lLqW08AHv27GHVqlU4Ozvz0ksvZc9JPCKD8e5LG7E7DzuLjEh2iLk40dohiJiJTgy5/yCRbJTbuYnVjl3+6SUW3d9/P2U+Md6+fTvz5883vT927BiJiYlUrFjR1EoTEBDAq6++ahpz5MgRevXqRUxMDJUrV6ZUqVIcPHiQoKAgfH19WbFiBZ6e5tNoG41GRo8ezbp168idOzf169cnJiaGXbt2kZKSwrvvvkvXrl0zfR7Z6bHswRcRERGRe8hBPfhhYWEcPHgwzfLUJ9YClC1r/nC/KlWqsGbNGj799FN27drFqVOnKFy4MP3792fIkCHpzoRjMBiYNWsWtWrVYtWqVfz+++84Ozvz5JNPMmDAAOrVq2f5k8siquA/BlTBl5xIFXzJaVTBl5zGqhX8zkstur//1vS26P7k3lTBFxERERFzOaeAL5mgBF9EREREzFn4QVeSvTSLjoiIiIiIHVEFX0RERETMqYJv05Tgi4iIiIg59XjYNP34RERERETsiCr4IiIiImJOLTo2TQm+iIiIiJhTfm/T1KIjIiIiImJHVMEXERERETNGB5XwbZkSfBERERExpx58m6YWHRERERERO6IKvoiIiIiYUwHfpinBFxERERFz6sG3aWrRERERERGxI6rgi4iIiIg53WRr05Tgi4iIiIg55fc2TS06IiIiIiJ2RBV8ERERETGnm2xtmhJ8ERERETGnBN+mqUVHRERERMSOqIIvIiIiImaMKuDbNCX4IiIiImJOLTo2TS06IiIiIiJ2RBV8ERERETGnB13ZNCX4IiIiImJOLTo2TS06IiIiIiJ2RBV8ERERETGnErBNU4IvIiIiIubUg2/TdH0mIiIiImJHVMEXEREREXO6ydamKcEXERERETNGtejYNLXoiIiIiIjYEVXwRURERMScSsA2TQm+iIiIiJhTD75N0/WZiIiIiIgdUQVfRERERMzpJlubpgRfRERERMypRcemqUVHRERERMSOqIIvIiIiIuZUwLdpSvBFRERExIxRLTo2TS06IiIiIiJ2RBV8ERERETGnCr5NU4IvIiIiIuY0TaZNU4uOiIiIiIgdUQVfRERERMypBGzTlOCLiIiIiDm16Ng0JfgiIiIikqOdPXuWwMBAdu/ezbVr13BycqJkyZK0atWKl19+GQ8PjzTbhIaGMnfuXLZv305oaCgFChSgSZMmDB06lPz581vhLLKPwWg0Gq0dhGStiIQN1g5BJA03p4LWDkHEjIuDp7VDELmLr9WOXHryRovu7/w7rTO97d69e+nXrx9xcXGULl0aPz8/YmNj2b9/P1FRUZQrV46VK1eSN29e0zZBQUF0796d69evU7ZsWfz8/Dh58iRnz56lUKFCfPvttxQpUsQSp5YjqcNKRERERMw5GCz7egSTJk0iLi6OIUOGsGHDBubMmUNgYCBbtmyhcuXKnDlzhi+++MJsm/Hjx3P9+nV69OjB+vXr+fjjj1m/fj09evQgJCSEt95665FiyumU4IuIiIhIjnTz5k1Onz6Ns7MzgwcPxnDHvQFeXl707dsXgIMHD5qWHz16lL///hsvLy/Gjx9v2sZgMDB+/Hi8vLz4888/OXHiRPaeTDZSgi8iIiIiZowGg0VfmeXs7PxA4/Lly2f6etu2bQA0a9YMV1dXs3Gurq40a9YMgM2bN2c6rpxOCb6IiIiImHOw8CuTcufOTY0aNUhMTOSzzz7jzltHw8PD+eqrrwDo2rWrafnx48cBqFKlSrr7rFy5MgAnT57MfGA5nGbREREREZEc67333qN///7Mnz+f9evX4+fnR1xcHPv27cPNzY0PPviAhg0bmsZfuXIFgEKFCqW7v8KFCwO3b8S1V0rwRURERMSchefBb968+T3Xb9myJcN1qbPkDB8+nAMHDnD+/HnTuvr161O+fHmz8TExMQC4u7unu7/U5dHR0Q8Suk1Si46IiIiImMtBs+j8/fffPP3000RGRvLFF1+wZ88efv/9d6ZMmcKuXbt4/vnn+fPPPy104vZBFXwRERERyVL3qtDfS3h4OMOHDychIYHAwECKFSsGQJ48eejevTuenp6MGDGCd955h02bNuHo6Giq0KdW8u+Wujy9h2PZC1XwRURERMRcDqngb9++nfDwcKpXr25K7u/UqlUrnJ2duXz5MpcuXQKgaNGiAISEhKS7z+DgYIB092cvlOCLiIiIiDmDhV+ZlJqke3qm/6RpJycnU8U+IiICgIoVKwJw5MiRdLc5evQoAH5+fpkPLIdTgi8iIiIiOVLBggWB20l5UlJSmvXnz583JfapFfmmTZsCsHXrVuLj483Gx8fHs3XrVgBatGiRZXFbmxJ8ERERETFjdDBY9JVZjRs3JleuXAQFBTFr1iyzJD8sLIy33noLgLp161KgQAHg9jz3Tz31FOHh4UybNs00d77RaGTatGmEh4fTsGFDKlSo8AjfoZzNYLzziQFilyISNlg7BJE03JwKWjsEETMuDum3AIhYj6/Vjlzyo+0W3d/FEU0yve2qVauYOHEiKSkpFC1alEqVKhEXF8fBgweJjIykQIECLF++nDJlypi2CQoKonv37ly/fp1y5crh5+fHyZMnOXPmDD4+Pnz33XcUKVLk0U8sh1KC/xhQgi85kRJ8yWmU4EvOowQ/1cGDB1myZAn79+8nNDQUR0dHihcvTuPGjenfvz/58+dPs01oaCiffvop27dv58aNG+TPn58mTZowbNiwdMfbEyX4jwEl+JITKcGXnEYJvuQ8VkzwP9lh0f1dHB5g0f3JvWkefBERERExZ9kH2Uo20022IiIiIiJ2RBV8ERERETHjoBKwTVOCLyIiIiJmDGrRsWm6PhMRERERsSOq4IuIiIiIGVXwbZsq+CIiIiIidkQVfBERERExY1AJ36YpwRcRERERM8rvbdsDJfjNmjV76Cs5g8HA5s2bMxWUiIiIiIhkzgMl+HXr1tVHNSIiIiKPCaV9tu2BEvwZM2ZkdRwiIiIikkMYNA2LTdOPT0RERETEjmQ6wY+KimLhwoX069ePzp07c+jQIQDCw8NZtGgRFy5csFiQIiIiIpJ9DAbLviR7ZWoWneDgYF588UWCg4MpVaoUZ8+eJTo6GgAvLy+++eYbgoKCeOuttywarIiIiIhkPQcl5TYtUwn+Bx98QHR0NGvWrMHb25v69eubrW/RogXbt2+3RHwiIiIiIvIQMtWi89dff9GrVy/Kly+f7uw6JUqU4OrVq48cnIiIiIhkP7Xo2LZMVfDj4uLw9vbOcH1qu46IiIiI2B4l5bYtUxX8cuXKsWfPngzXb968mUqVKmU6KBERERERyZxMJfgvvfQS69evZ+HChURFRQFgNBq5cOECY8aM4cCBA/Tp08eScYqIiIhINjEYDBZ9SfbKVItOp06duHLlCp988gkff/wxAP3798doNOLg4MCIESNo0aKFJeMUERERkWyiB13Ztkwl+ACDBw+mU6dObNq0iQsXLpCSkkLJkiVp1aoVJUqUsGSMIiIiIiJ24/Dhw+zatYvg4GDi4uKYNm2aad21a9dISkqiaNGimd5/phN8gKJFi6oVR0RERMTOqKsma1y/fp033niDv//+G7jd4m4wGMwS/I8//pjVq1ezcuVKqlevnqnjPFKCf+rUKXbs2EFQUBAAxYsXp1GjRvj5+T3KbkVERETEipTgW150dDS9e/fm3LlzFCpUiPr167Nr1y5CQkLMxnXu3Jkff/yRLVu2ZG+Cn5CQwMSJE1m7dq2p7x4gJSWF2bNn07FjR6ZOnYqLi0umghIRERERsSeLFi3i3LlzBAQE8OGHH+Lh4UHPnj3TJPi1atXCxcWFXbt2ZfpYmUrwZ86cyZo1a+jZsycvvvgiJUuWxGAwcOHCBZYtW8bKlSvJmzcvEyZMyHRgIiIiImIdquBb3qZNm3BycuK9997Dw8Mjw3GOjo6UKlWKS5cuZfpYmUrwf/rpJzp16sTEiRPNlpctW5Z33nmHqKgofvrpJyX4IiIiIjbIQQm+xV26dImSJUtSoECB+4718PB4pAfHZmoSpKSkJKpVq5bh+ho1apCcnJzpoEREREREHlcRERG4u7tnevtMJfgNGzbkzz//zHD9H3/8QYMGDTIdlIiIiIhYj8Fg2Zfcnozm8uXLxMbG3nNcaGgoFy5coGzZspk+1gMl+OHh4Wav4cOHc/nyZV577TV27dpFUFAQQUFB7Ny5k1dffZUrV64wfPjwTAclIiIiItajBN/yGjVqRGJiIoGBgfccN2fOHIxGIwEBAZk+1gP14D/11FNpHjNsNBo5deoUW7ZsSbMcoEOHDhw7dizTgYmIiIiI2IuXX36Z7777js8++4zo6Gh69Ohhtv7kyZN8+eWX/PTTT3h7e9OzZ89MH+uBEvxXX301TYIvIiIiIvbJoLtsLa5gwYJ8+umnvPbaayxdupSlS5ea1lWqVAmj0YjRaMTDw4NPPvmEvHnzZvpYBmNqyV3sVkTCBmuHIJKGm1NBa4cgYsbFwdPaIYjcxddqR667KuN7LTPjn64NLbo/W3bx4kXmzJnDli1bzPrxXVxcaNKkCSNGjKBMmTKPdIxHepKtiIiIiIg8uJIlSzJr1iySkpK4cOGCacacMmXK4OrqapFjPFKCv2/fPo4dO0ZkZCQpKSlm6wwGA6+++uojBSciIiIi2U+d2VnPycmJcuXKZc2+M7NReHg4AwcO5NChQxiNRgwGg+nm2tSvleCLiIiI2CYl+LYtUwn+Bx98wMmTJ5k9ezZVq1alRYsWfPnllxQvXpzFixdz4MCB+04BJCIiIiJij/bs2WOR/dSpUydT22Uqwf/999/p3r077dq14+bNmwA4ODhQqlQp3nnnHV577TWmTZvGhx9+mKmgRERERMR6NInOo+nVq9cjz0BpMBgyPeV8phL8W7duUb58eQA8PDwAiI6ONq1v0KABH330UaYCEslOoaG32LPrJMePXeL40YucPBFEXGwCRYp6s3bjOxluN3nC1/zy0z/33Pebb3fl2W6aNUAsJyjoOm1aPNhDBIsVK8iGzZ9kcUQicPLkebZs2c3evUc4deoC4eGRuLq6ULp0UZo2rUuvXh3Jmze3tcOUh6QWnUdTtGhRqx4/Uwm+j48PoaGhwO0pffLnz8+JEydo0aIFACEhIZo3X2zCb7/u56MPVmd6+0KFvShcJF+66woUyPz8tSLpcXVxpkbNe0+bd/DAaVJSjNSo6ZdNUcnj7OLFqzz99FDTex8fb/z8SnP9+k2OHPmPI0f+45tvNvDll5Px8yttvUBFstnWrVutevxMJfh16tRh586dDB48GIC2bdvy5Zdf4ujoSEpKCkuWLKFRo0YWDVQkK3jkzkWdJ32pWLkEFSuXJPjqTT6ZteaBt+/4zFMMGNI26wIUuUOBgl4s/XpShuuPHztPt2fHA/DMs5l/xLnIgzIajXh75+WFF9rTqVNTSpQobFq3b98xxoyZTVDQNV599T3Wr5+Pi4uzFaOVh2FwsHYE8igyleD36dOHnTt3kpCQgIuLC0OHDuW///7jk09ufxxcp04dJkyYYNFAbcHixYspU6YMAQGW+8N6+fJlmjdvzieffEKbNm0stl+57elnnuLpZ54yvd/0634rRiPyaFb/uB2A4iV8qFO3knWDkcdC4cIF2LLlC9zdc6VZV6tWJWbOHEXPnm9y6VIwf/yxn+bNn7RClJIZasSwbZlK8P38/PDz+9/Hv3nz5mXx4sXcunULBwcHcud+PHvtli5dSpMmTSya4IuIPIiEhETWr9sJwDNdmqhNUrKFq6vLPdfXqlUJT08PIiOjOXPmkhJ8kWxi0SfZ5smTB4Cff/6Z1atX89VXX1ly93YjISEBJycnHBz0+Zet2/fPacb+F0xEeBS5Pd3wrVCMVm1rUaq0j7VDk8fM1s17iYiIwsHBwNOd1SIpOUNSUjJJSUkA6Vb5JedSkSDrhISE8PPPP3P8+HHCw8NJTExMd5zBYGDJkiWZOoZFE/xUly9fZteuXVmx64cyduxYjhw5wttvv8306dM5f/485cuXZ9KkSVSpUgW43T/41Vdf8d133xEUFEShQoXo1asXffr0SbOfdevWmZbdunWLOnXqMH36dLp06UKzZs0ICgri66+/5uuvvwYwW9ekSROKFCnCihUruHr1Kjt37uTmzZvMnTuX/fv3Ex4eTrFixXjuuefo06ePkn8b8e++M2bvd2w9zJcLNvJin2a8+npH/QMp2Sa1Pad+g6oULpzfusGI/L/Nm/8mNjYegDp1qlg5GnkY+vOVNb799lumTp1quvAFTA+Lhf9dWKU+NDazsiTBz0muX7/O1KlTGTBgAJ6ensyePZvXXnuN3377DWdnZ9577z1WrVrFoEGDqFatGvv372fWrFm4urry/PPPP/Bx5s6dy4ABA6hZsyZ9+/YFoGTJkqb1mzZtolSpUkyYMAEHBwfc3d05efIkZcqUoWPHjnh4eHD8+HE+/fRTYmJieO211yz+vRDLKVGqIMNGdaJ23ScoUsybXLlcuHAuhFXf/MnaH3ax9KstODo6MnhYe2uHKo+Bq1dC+XvXEQCeebaJdYMR+X+3bkXx/vtfAtC0aV3NoiOPvb179zJp0iRy5cpFv379+PXXX7l48SLvvfce4eHhHDx4kK1bt+Lk5MSQIUMoWLBgpo9l9wl+REQEy5cv54knngDAzc2N3r17c/DgQXx8fFi+fDmTJ0+me/fuANSvX5+4uDjmzZtH9+7dH7iSXqlSJVxcXChQoADVq1dPsz4xMZHAwEDc3d1Ny+rVq0e9evWA21dqtWrVIi4ujuXLlyvBz+H6DmiVZplvheJMmNSD4sXzM++TdSxbtIXOz9WjSFFvK0Qoj5M1q38nJcVIvnyeNG1ay9rhiJCUlMyIETO5cuU63t55mTx5iLVDkoekCr7lLV26FLjd4dGmTRv++ecfLl68yLPPPmsac+bMGQYPHsw333zD6tWZn8bb7vtAfHx8TMk9YHpAV0hICDt33r4hrVWrViQlJZle9evX5/r161y9etVicTz55JNmyT1AfHw8c+bMoWXLlvj7+1O5cmU++ugjrl+/bvbgMLEtL/RpRkGfvCQlJfP7tiPWDkfsnNFoZO2aHQC079gQZxe7r9tIDpeSksKbb37En3/ux8PDjQUL3qZQIbWN2RqDwbIvS4iMjOSTTz6hY8eO1KhRgxo1atCmTRsmTJhASEhImvEXL15kzJgxNGzYEH9/f1q2bMmsWbOslmMdOHCAPHny0Lp16wzHlCtXjjlz5nDlyhXmz5+f6WPZfYKfeuNvKmfn23PwxsfHc/PmTYxGI0899RSVK1c2vV5++WUAiyb4+fOn/cdt5syZfPnll3Tt2pWFCxfy/fffm54tEB8fb7FjS/ZycnKksn8pAC5dvG7laMTe/bP7GEGXb/+edVF7jliZ0WhkwoQ5rFu3A3f3XCxc+A7Vqumha/Lo/vvvP9q1a8f8+fOJj4+nUaNGPPXUUzg6OvL9999z6dIls/FHjx6lc+fO/PTTT/j4+NC8eXOSk5MJDAykR48eREZGZvs53Lx5k6JFi5p66x0dHQGIi4szG1ehQgXKlCnDtm3bMn2sBy71dOzY8YF3GhYWlqlgslvevHkxGAysWLHClPjfqUyZMsDtp/XefYdzRETEQx0rvRslNmzYQPfu3RkwYIBp2Y4dOx5qv5IzOTvf/p82KSnZypGIvUu9uda/ajme8C1h3WDksWY0Gnn77bn8+OMW3NxcWbBgIrVrV7Z2WJJJDjmoRefWrVv07duX8PBwZs2alSYnvXjxotkU7cnJyYwcOZLo6GhGjRplyrMSEhIYNmwY27ZtY+bMmUyZMiVbzyN37txmN9SmFqGvXLlC2bJlzca6uLhw/vz5TB/rgRN8Ly+vB96pl5dXmkBzotT+9/DwcJo1a5bhuMKFCxMcHEx0dDQeHh4A/PXXX2nGOTs7P1TlPT4+3uzCIjk5mV9++eWBt5ec67/Ttz/9KVTYy7qBiF2LjIxhy297AOjcpYl1g5HH3uTJn7Fq1SZy5XLhs8/e5skn/a0dkjyCnJTgz507l5CQEMaNG5duwfnOSU0AtmzZwvnz5/H19eWVV14xLXdxcWHKlCk0bdqUH374gREjRpAvX74sjz9V4cKFuXbtmul9uXLl2Lp1K3/99ZdZ3nz9+nXOnTuHm5tbpo/1wAn+smXLMn2QnKpMmTK88MILvPHGG/Tr149q1aqRmJjI+fPn2b17t6n3qVWrVsyZM4fx48fTrVs3Tp8+zffff59mf2XLluXvv//mr7/+Ik+ePBQvXvyevzj169dn1apVlC9fnnz58rFixQoSEhKy7Hwle/yx/QjnzgQD8FT9ilaORuzZ+nV/EReXgJubK+3a17N2OPIYmzr1c1au/BVXVxfmz3+LevWqWTsksRPx8fH8+OOPuLm5mSZEuZ/U1pbWrVun6aDw8fGhVq1a7N69mx07dtC5c2dLh5yhGjVq8M0333D9+nUKFixIixYtWLhwIbNnz8bJyYnatWtz/fp1PvzwQxITE2ncuHGmj2X3Pfj389Zbb/H666+zfv16BgwYwBtvvMGvv/5K3bp1TWPKly/PjBkzOH78OEOGDOH3339n1qxZafY1cuRIChcuzNChQ3nuuefu2zv19ttvU6dOHd59910mTJiAr68vgwYNsvg5imXt3nmCObPXcuGc+Q09yckprP95DxPH3r5LvnFTfypWVsuEZJ3VP95u6WvRsg65c7vfZ7RI1vjgg0UsW7bu/5P7CTRoUMPaIYkFOBiMFn1l1pEjR4iMjKRSpUq4ubmxa9cuPvjgAyZOnMjChQs5e/Zsmm2OHz8OYHrm0d0qV77dOnbixIlMx5UZAQEBpKSksH37dgCqVq3K008/TVxcHFOmTOHpp5+mX79+HDlyBDc3N4YPH57pYxmMdzYDiV2KSNhg7RByrJDgm7zYdabpfVJiEtHR8Tg4GPDM87+EqVr1Msz69PbHfNu3HOKN12/P7ZzPOzeFCufDYIBLF0OJiowFoFad8syc8wq5c+vJjRlxc8r8/L4Cp09dokunNwH4asnb1KmrT4selYuDp7VDsDn//nuCHj3GAJA/vxelShXJcGxAQG0GDeqWXaHZCV+rHbntpj8tur9fWzXM1HbffvstEydOpFWrVhgMBjZu3Gi23sHBgUGDBpklw3Xr1iUiIoK1a9dSoUKFNPtcvHgx06dPp3Xr1syZMydTcWVWTEwMzs7Ophbt5ORkvvzyS1avXs3ly5dxc3OjTp06DB8+HF/fzP/8NZ+aPNaSk1OICE87XVZKitFseVTU/+5wr1i5BP0GtubI4QtcvHCNi+evkZiYRF4vD6rXLEub9rVp0bq6nkYsWerHH7YDUKJkIWrXSfsHTCQ7JCT8bwKKGzfCuXEjPMOx90r+xf41b978nuu3bNmS7vLUSU1SuyLGjBlDx44dcXR05Ndff+WDDz5g/vz5FC1alK5duwK3k2ggwx721PsprTFd5t1Tpjs6OjJgwACzCVcsQQm+PNaKFsvPP4c/eahtChXOx8DX2mVRRCIP5s1xvXhzXC9rhyGPuSef9OfkyZ+tHYZkgZxSokpJSQFuPzB06NCh9O/f37SuV69eJCUlMWPGDObPn29K8EUJvoiIiIjc5VH65tOTUYX+fu6seKeXwHfr1o0ZM2Zw5coVLl26RIkSJXB3dyciIoLY2Nh095lauU+t5GeXxMRErl+/jpub2z0nYbl58yaxsbH4+Pjg5JS5VD2nXKCJiIiIiJgpVqwYcHuKy0KFCqVZ7+Hhgbe3N3B7ekmAokWLAhAcHJzuPlOfeps6Lrv88MMPNG/enB9//PGe43788UeaN2/OmjVrMn2sR0rwQ0JCWLduHUuWLDF9E5OTkwkPDyc5WQ/4EREREbFFDgbLvjKrUqVKwO2HVKXXM5+cnGx6Km1qtb9ixduTDhw5ciTdfR49ehQg3Rtws9Jvv/2GwWDgmWeeuee41Kk7776h+GFkKsE3Go1Mnz6d5s2bM3r0aGbMmMG5c+eA2zc2NGvWzC7nzRcRERF5HDhY+JVZRYoUMU1ruXv37jTr9+7dS2JiIm5ubqaHRTVt2hS4nSDfPVnktWvX2LdvH05OTo80z3xmnD17loIFC5o+cchI/vz58fHx4cyZM5k+Vqa+51988QVLly6lb9++LFq0yOyb5+npSatWrdi0aVOmgxIRERERAUwzzHzwwQdcvnzZtDwkJIT33nsPgOeeew4XFxcAmjVrRunSpTl16hSBgYGm8QkJCUycOJGkpCSeffbZ+ybalhYaGppum1F6fHx8uHHjRqaPlanO/VWrVtG5c2dGjhzJzZs306z38/Pj999/z3RQIiIiImI9j9JWY2lt2rTh+eefZ+XKlXTs2JGaNWvi4ODAv//+S2RkJNWrV2fUqFGm8U5OTsyePZtevXoxe/ZsNmzYQKlSpTh48CBBQUH4+voyZsyYbD8Pd3d3QkNDH2jsjRs3cHV1zfSxMlXBv3r1KjVqZPykOjc3N6KiojIdlIiIiIhYj8FgtOjrUU2aNIlZs2bh5+fHv//+y+7duylatCijRo1i6dKlaea8r1KlCmvWrKFjx46EhITw22+/4eDgQP/+/fnmm2/w9Mz+B9v5+vpy9epV05N2M3L8+HGuXLlCuXLlMn2sTFXw8+fPz9WrVzNcf/ToUYoU0QMtRERERMQyOnbsSMeOHR94fKlSpZg1a1YWRvRwWrZsyZ49exg3bhyLFi1Kd6rM8PBwxo0bh8FgoHXr1pk+VqYS/JYtW/LNN9/QpUsXcufODYDBcPuznD///JPVq1fTr1+/TAclIiIiItaTk1p07EX37t1ZuXIlJ0+epH379nTr1o0aNWrg6elJZGQk+/fv5/vvv+fGjRuULVuW559/PtPHMhjvvr34AURGRvLCCy9w+fJlateuzR9//EH9+vWJiYnhwIEDVKxYka+//jrDRwRL9opI2GDtEETScHMqaO0QRMy4OGT/R/Yi9+ZrtSP32GbZeym/aZq9M9bkVJcuXWLgwIGcPXvWVBy/k9FopHz58nz22WeUKFEi08fJVIIPEBcXx1dffcXGjRu5cOECKSkplCxZkjZt2tC/f39y5cqV6aDEspTgS06kBF9yGiX4kvMowbdH8fHxfPfdd2zatIlTp04RHR2Nh4cHfn5+tG7dmq5du5pmBMqsTCf4YjuU4EtOpARfchol+JLzWC/B77l9h0X3t6JJgEX3J/eWqR58EREREbFf6sHPPocOHeLAgQMkJiZSunRpGjVq9MgV/Ewl+OPGjbvvGIPBwLRp0zKzexERERERm3b16lXWrl1Lnjx56NmzZ5r1sbGxjBgxgh07zD8tKVasGHPnzqVChQqZPnamEvz0HhWckpLC9evXSU5OxtvbWzfYioiIiNioTD0oScxs3bqVTz75hD59+qS7fsqUKWzfvh0ABwcHvL29uXHjBpcvX2bgwIH8+uuvuLu7Z+rYmUrwt27dmu7yxMREvv32W5YsWcJXX32VqYBERERExLrUovPo9u7dC0C7du3SrLtw4QJr1qzBYDDQsmVLpk2bRu7cuTl79iyvvvoq58+fZ9WqVbz00kuZOrZFL9CcnZ158cUXadCgAe+++64ldy0iIiIiYjP+++8/3Nzc8Pf3T7Nu48aNGI1G8ubNa0ruAcqWLcv48eMxGo2m6n5mZMknMBUqVGDPnj1ZsWsRERERyWIOBqNFX4+jGzduUKpUqXTX7d27F4PBQJMmTUzJfapGjRqRN29eTp8+neljZ0mCv3PnTvXgi4iIiNgoB4NlX4+jW7du4eCQfqp97NgxAJ588sl01xcuXJhbt25l+tiZ6sGfO3duussjIyPZs2cPx44dY8CAAZkOSkRERETElrm5uXHt2rU0y4ODgwkNDcVgMFCpUqV0t3VycuJRHlVl0QQ/b968lChRgsmTJ9OtW7dMByUiIiIi1qNZdB5dmTJlOHz4MEePHqVy5cqm5anTYnp4eODrm/7DzEJCQvDy8sr0sTOV4J84cSLTBxQRERGRnO1x7Zu3pEaNGnHo0CHeffdd5s2bR/78+bl48SILFy7EYDDQtGlTDIa0/UshISGEhoZSp06dTB/7oS/Q4uLimD59eoZTZYqIiIiIPO5efPFFvLy8OHjwIAEBATRs2JDWrVsTFBSEg4MDL7/8crrbbdy4EYC6detm+tgPneDnypWLb7/9lhs3bmT6oCIiIiKSc+km20eXL18+FixYQP78+UlKSiI0NBSj0YijoyPjxo1Lt//eaDTyzTffYDAYqF+/fqaPnakWncqVK3Pq1KlMH1REREREcq7HNSm3tOrVq7Np0yZ27NjBpUuX8PDwoHHjxpQoUSLd8eHh4bzwwgsA1KhRI9PHzVSCP378eAYMGICvry/PPPMMTk6Z2o2IiIiIiF1zd3enbdu2DzQ2X758pgT/URiMDzgHz549eyhXrhze3t507NiRmzdvcuPGDVxcXChUqBCurq7mOzYY+Omnnx45QHl0EQkbrB2CSBpuTgWtHYKIGRcHT2uHIHKX9GdYyQ7Ddm2z6P7m1Gtq0f3JvT1w6b13797MnDmTDh064OXlhZeXF2XKlMnK2ERERETECjSLjm174ATfaDSaJtxftmxZlgUkIiIiIiKZp+Z5ERERETGjm2xt20Ml+OlNxi8iIiIi9kVPsrVtD5XgjxkzhjFjxjzQWIPBwLFjxzIVlIiIiIiIZM5DJfj169endOnSWRSKiIiIiOQEatGxbQ+V4Hfu3JmOHTtmVSwiIiIikgMYNIuOTVOLlYiIiIiIHdEsOiIiIiJiRi06tk0JvoiIiIiYUYuHbXvgBP/EiRNZGYeIiIiIiFiAKvgiIiIiYsZBN9naNCX4IiIiImJGPfi2TS1WIiIiIiJ2RBV8ERERETGjCr5tU4IvIiIiImYcrR2APBK16IiIiIiI2BFV8EVERETEjGbRsW1K8EVERETEjHrwbZtadERERERE7Igq+CIiIiJiRhV826YEX0RERETMOCrBt2lq0RERERERsSOq4IuIiIiIGbXo2DYl+CIiIiJiRtNk2ja16IiIiIiI2BFV8EVERETEjFp0bJsSfBEREREx42jtAOSRqEVHRERERGyG0Wikd+/e+Pn54efnx5kzZ9Idd/HiRcaMGUPDhg3x9/enZcuWzJo1i+jo6GyOOPspwRcRERERMw4Gy74s6dtvv2X37t0YDBnv+OjRo3Tu3JmffvoJHx8fmjdvTnJyMoGBgfTo0YPIyEjLBpXDqEXnMbD0P13HSc4zpKKztUMQMVOu1nprhyBi5sw+X6sdO6fOohMcHMzMmTNp1KgRZ8+eJSgoKM2Y5ORkRo4cSXR0NKNGjWLAgAEAJCQkMGzYMLZt28bMmTOZMmVKdoefbZT5iYiIiIhNmDhxIikpKUyePDnDMVu2bOH8+fP4+vryyiuvmJa7uLgwZcoUnJyc+OGHH7h582Z2hGwVSvBFRERExIyjwbIvS1izZg07duxg+PDhFCtWLMNx27ZtA6B169Zp2nh8fHyoVasWSUlJ7NixwzKB5UBK8EVERETETE7rwQ8NDWX69On4+/vTu3fve449fvw4AFWqVEl3feXKlQE4ceLEoweWQynBFxEREZEcbcqUKURFRTF16lQcHO6dvl65cgWAwoULp7u+UKFCZuPskW6yFREREREzlp75pnnz5vdcv2XLlgzXbdy4kY0bNzJgwAAqVKhw32PFxMQA4Obmlu56Dw8PALueLlMJvoiIiIiYySlPsg0PD2fKlCmUKlWK1157zdrh2Awl+CIiIiKSpe5Vob+X6dOnExoayqxZs3B1dX2gbdzd3YmIiCA2Njbd9amV+9RKvj1Sgi8iIiIiZhxzyDz4W7ZswdXVlfnz5zN//nyzddevXwfgzTffxM3NjRdeeIE2bdpQtGhRIiIiCA4OTrelJyQkBICiRYtm/QlYiRJ8ERERETGTk2ZhiY+P559//slw/eHDh4H/9flXrFiR48ePc+TIEZo0aZJm/NGjRwEeqJ/fVinBFxEREZEcae/evRmua9asGUFBQaxfv55y5cqZljdt2pQff/yRjRs38uqrr5rNhX/t2jX27duHk5MTjRs3ztLYrSknXaCJiIiISA6Q0+bBfxjNmjWjdOnSnDp1isDAQNPyhIQEJk6cSFJSEs8++yze3t7ZG1g2UgVfRERERMzklFl0MsPJyYnZs2fTq1cvZs+ezYYNGyhVqhQHDx4kKCgIX19fxowZY+0ws5Qq+CIiIiJiV6pUqcKaNWvo2LEjISEh/Pbbbzg4ONC/f3+++eYbPD09rR1illIFX0RERETM5JRZdO5l69at91xfqlQpZs2alU3R5CxK8EVERETEjC236IhadERERERE7Ioq+CIiIiJiRhV826YEX0RERETMKMG3bWrRERERERGxI6rgi4iIiIgZR1XwbZoSfBEREREx42AD02RKxtSiIyIiIiJiR1TBFxEREREzqgDbNiX4IiIiImJGs+jYNl2giYiIiIjYEVXwRURERMSMZtGxbUrwRURERMSMZtGxbWrRERERERGxI6rgi4iIiIgZ3WRr25Tgi4iIiIgZJfi2TS06IiIiIiJ2RBV8ERERETGjCrBtU4IvIiIiImYMatGxabpAExERERGxI6rgi4iIiIgZFfBtmxJ8ERERETGjFh3bphYdERERERE7ogq+iIiIiJhRBdi2KcEXERERETMGg9HaIcgj0AWaiIiIiIgdUQVfRERERMzoHlvbpgRfRERERMxoFh3bphYdERERERE7ogq+iIiIiJhRAd+2KcEXERERETMOyvBtmlp0RERERETsiCr4IiIiImJGBXzbpgRfRERERMxoFh3bphYdERERERE7ogq+iIiIiJhRAd+2KcEXERERETNK8G2bWnREREREROyIKvgiIiIiYkbz4Ns2Jfgid4gMvcmKYdNIjI0DoPfnk8jjk99szPm9R7iw/xjXzlwi6kY4cbeiMDg4kLuAF8X9fanWoSn5ivlYI3x5DMz99Dvmz1t1zzH9X+nEyFEvZlNEYg8K5HenQd2S+FcqhH8lHyr5+eDu5szlK7cI6PjVfbcPqF+aPs9Xp0qlQrjncuZK8C1+236WzxbtITIqPsPtSpf0ou8LNalfpwRFCnliMMD1GzHsPRDEkm8OcOhoiCVPUx6C8nvbpgRf5A7b5q80JfcZOfDzNi4fOoWDowPu+fKSv1RR4qNiuBUcypGgaxzb/Dcthr2Ib6Na2RS1PI7y589LqVKF011XVBeY8pA6tPLj7dEBmdp2+MCnGDbgKQCCr0Vx5eotypXxZmCf2nRo7Uu3vt8RfC0qzXbNGpVh7vvtcXV1Ij4hiUtBt0hKSqZkcS86t6vI020qMGXmdpZ9d/CRzk3kcaQEX+T/HdvyNxf/PU7ZJ6tydvehDMdVaPokNZ9pSdGKZXFydTEtj7oRzu+Bqzi7+xBb535N0YplyV0gX3aELo+hRo2qM23Ga9YOQ+xEVHQ8f+2+yOHjIRw+FkLRwp5MGHn/hL9Jg9Km5H7S+9tMyXjePK58OqM9DZ4syZzp7ejW7zuz7dzdnJk5uTWurk5s//Mcb075jdAbMQDk9nBh/IhGdH/GnwmjGrNj53kuXo6w8BnL/RgMRmuHII9AN9new9ixY+nQoYO1w5BsEB0WwV+LfsTTx5sne977Z16hSV1KVq9gltwD5M7vRauRfXD1cCMpIZHze49mZcgiIhbz/U/H6D3kR2Z++hcbtvzHtevRD7Td64PqAbD21xNmlfaIW/EMH7+eyKh4alUvSuN6pcy2q1uzGF55c5GSYmTk2xtMyT1AVHQCb03bSsj1KJydHAmoX/rRT1AemsHCL8leSvBFgO2ff0t8dCxNB/fAOZfL/TfIgJOLM3kKFQAgMT7jvlMREVtXolge/CsVAuDr79N+6nkzPI4NW/4DoENrP7N1uXLdbiAIvxVHxK20/1ampBi5HHQLACcnpSoiD0stOvLYO/XHXs79cxi/gDqUrF6RW9duZHpfsbeiuBl0+6Ywn/Kl7jNaJPNOnLzAmNGfEHr9Ju7uuShXvgRt2tSjUuWy1g5NHhM1qxYFID4hiYNHgtMd88/+y3TtVJmaVYuYLT9+8jopKUa8vdwoVzofZ87fNFufN48rvuVvT3CQ0b4laxlUdrdpuix+ALt376Zz585Ur16d5557jiNHjpjWxcfHM336dBo2bIi/vz+dOnXit99+M9s+tdVnx44ddOjQAX9/f7p06cKBAwfSHOvHH3+kY8eO+Pv706hRIz766COSk5Oz+hQfW7ERkfz+xQ/kypObhn27PNJ+Luw7yk+T55EUn4Bv49oUq1zegpGKmDtx/Dy/rPuT3buPsm3bPr4IXMNzz77J+HHziI9PsHZ48hgoU8oLgCtXI0lKSkl3zIX/750vUSwvjo4Gs+WpVf/PP3yaZo3KkDePKx7uztStWYwv53TGM7crq9YeZf+hq1l7IpIuBwu/JHupgn8f169fZ+rUqQwYMABPT09mz57Na6+9xm+//YazszOjR4/mjz/+4PXXX6ds2bKsXbuWoUOHMm/ePJo3b262n8mTJzN06FDy5MlDYGAg/fr1Y9OmTeTPf7tKsWjRImbOnMlLL73E2LFjOXPmjCnBHz16tLW+BXZtR+Aq4m5F0fL13rjlyf1Q257dfZD1M74wW5anUH6aDO5B5Zb1LRmmiImPTz5efa0bDRtWo3iJQnh6unP58jXWrN7Goq9+Zs3q7SQlJfPBzGHWDlXsXN48uYDbbTYZiYi4vc7JyYHcHi5m7TiT3t/GuQs36d29GoEfdzLb7lJQBG9M2sQPPx/LgshF7J8S/PuIiIhg+fLlPPHEEwC4ubnRu3dvDh48SO7cudm0aROTJ0+mR48eADRu3JigoKA0CX54eDgff/wx9erdviGpbt26BAQEsHjxYkaNGkVUVBRz5syhf//+jBw5EoAGDRrg7OzMjBkz6NevH/nyaUYWSzq7+yD//fUvJWtUxC+gzkNvn8vTgyIVymI0GokOiyDqRji3roVx6ve9FKtUjnzF05/CUORRdOveMs2ysmWLMXLUi1SoUIbRoz5m3c9/0LNna6rX8EtnDyKWkcv1dgqRmJjxp8zxCUmmr91yOZsl+LlyOVGsSB7y5MlFUlIKQVdvERuXSKkSXpQolpeunSrz7+GrnL2rfUeyh1p0bJsS/Pvw8fExJfcA5cvfbrsICQnh5MmTALRp08Zsm7Zt2zJ9+nRiYmJwd3cHwNPT05Tcp76vX78+Bw/ennXg33//JSYmhjZt2pCU9L9/EOvXr09cXBynT5+mbt26WXOSj6G4qBi2f/4dzrlcaDKoR6b2UbRSeZ6dPsL0Pjosgr9XrOP4lr9Z9eZsenw0jjw+3pYKWeS+2rVvwJIl6zh86D82bvxbCb5kqbj423+rnJ0dMxzj6vK/NCM2LtH0tZOTAysXPkfVyoX5fdcFxr/7G1dDbs+V75bLiRGD69PvxZp8v6g77XssN62T7JNT8vvExER2797N9u3b2b17N5cuXSI5OZnChQvTsGFD+vfvT7FixdLd9uLFi3z66afs2rWLiIgIChcuTOvWrRk8eDAeHh7ZfCbZS21R95EnTx6z987OzsDt3vuIiAicnZ3x8vIyG1OgQAGMRiORkZGmZd7eaRO9/Pnzc/36dQBu3rxdoXjmmWeoXLmy6dWqVSsArl5VD6Il7Vyyhpibt3iyZweLJeEe3nlp/toLlKhegYSYOPZ+v9Ei+xV5GDX/P6m/eEH/ZkjWuhV5uxqfL2+uDMfk/f91SUkpREX/796QHs9UoWrlwoSFxzJs7HqzBD42LolpH/3Orj2XyJsnF4P7qrj1ONuzZw/9+vVj2bJlREZG0qBBAxo3bkxcXBwrVqzg6aef5t9//02z3dGjR+ncuTM//fQTPj4+NG/enOTkZAIDA+nRo4dZjmaPVMF/BHnz5iUxMZGIiAjy5s1rWh4aGorBYMDT09O0LCwsLM32N27coGDBgqZ9AcydO5fChdO2dhQvXtzS4T/Wrv13EYB9329i/4/mN0WnpPzv4R7fjZmFg4OB8g1q0rj/cw+07zJ1qnDpwAmun7louYBFHpDz/1dME5N0c75krdTWmSKFPXFyckj3RttSxW//bbsUFEFy8v/+ba1b8/bftINHgomMSn9K4d93nqdenRJU/f+pOCV75ZQWHYPBQOvWrXn55ZepUaOGaXl8fDyTJk3ixx9/ZNSoUWzcuNFUhE1OTmbkyJFER0czatQoBgwYAEBCQgLDhg1j27ZtzJw5kylTpljlnLKDKviPoFatWgBs2LDBbPmGDRuoVKmSqT0HIDIykl27dpm937lzJ9WqVQOgRo0auLm5ERwcjL+/f5qX+u+zRuytKGLCI81ecbf+V0mK+//1CTGxD7zPlOTbf+RSUtKfVUIkK506dfvCskjh/FaOROzdv4dvf0rk6uJE9Srp33OUmsinjk2V2+PBnzfi6pJxC5BknZzyoKt69eoxZ84cs+QewNXVlXfeeQdPT0+CgoLMqvhbtmzh/Pnz+Pr68sorr5iWu7i4MGXKFJycnPjhhx9M3RP2SBX8R1ChQgVatWrFjBkziIuLo0yZMvz000/8+++/zJ8/32ysl5cXEyZMYNiwYXh6ehIYGIjRaOSll14CbrcCDRs2jJkzZxIcHEzdunVxdHTk0qVLbNmyhU8//RQ3NzdrnKZd6vHR2AzX3bp2g6UDJwHQ+/NJ5PF5uETpzK4DABQso09dJHsdP36Ov/68fV9Pg4bVrRuM2L2LlyM4cjyEKhUL0fO5quw9cMVsfT6vXLRpfvu+tV82nTJbd/bCTQIalKZalcJ45nZNt4rf+P+fYHv3HPkiqXLlykXp0qU5fPgw165dMy3ftm0bAK1bt8Zw10cRPj4+1KpVi927d7Njxw46d+6cnSFnG1XwH9HMmTPp2rUrgYGBDBkyhFOnTjFnzhyaNWtmNq5gwYJMnDiRhQsXMnz4cOLj4/nyyy8pUKCAaUzfvn2ZPn06u3fvZtiwYQwfPpzvvvsOf39/08dOYl0h/13k76/XmR5mdafI62Fs/HAxV4+fxeDgQLUOTbI/QLFrp09f4p23F3Ds2Lk063Zs38fAAdNITk6hUqUytGipvmXJeh9//jcAndpWoFe3aqblefO48sm0dnjmdmX/oats/+u82XarfzlmetDVnOltKezzv2mK3XI5MX5EY+rVKQHAD+s0VaY1OBgs+8oKycnJBAUFAZjlU8ePHwegSpUq6W5XuXJlAE6cOJE1geUAquDfw4wZM9Isy5Mnj2n2HLh99Th+/HjGjx9/3/01adKEJk2a3HNM+/btad++/UPHKtkjMS6evd9vZO/3G8nl6YFnwXw4ODkRGxHJrWthYDTinMuFZq/2pGDZEtYOV+xMUlISq1ZtYdWqLeTJ40Gx4j44OTkSdPkaYWG3APDzK8Xc+W/i4KD6jTy4IoVy89OKF0zvnZ0cTMv3bBloWr7vwBUGjfrZ9H7bH+eY/+U/DOlXl0lvNmXQy3UIvRFNuTLeuOVy5krwLYaNW5/meEdPXOf9OX/w5rBGNK5fmh0/9+XylQji4pMoVcILt1y3i1pfLN/Htj/SXtBK1rN0Tn7n1OHp2bJly0Pvc+3atYSFheHt7U3NmjVNy69cuf1pUnr3NAIUKlTIbJw9UoIv8hAKlC5G41eeI+jIf9y4cIWI4Bskxcfj7JaLQk+UokQ1P6q0akDuArpnQiyvWDEfhr3eg0MHTnPm7GUuXQwmLj6BPJ4e1KvvT+vW9ej8TBNcXPSJnzwcBwcHvL3StoE6Opov98ztmmbM7Pk72XfoCn2er0GVCj48UTY/V0Mi+W37GeZ/tcc0287dvli2nz3/XqFX16rUql6MIoU8wQBhYbFsO3KOb348wl+7NVmBpO/y5cu8//77AIwYMQIXl//d1xETEwOQYWtz6hSZ0dHRWRyl9SjBF7lLHp/8vLb603TX5crtTtV2AVRtF5DNUYlAnjweDBr0rLXDEDsUdPUW5Wp9nOntt/95nu1/nn/o7Q4eCebgkeBMH1eyjsFgvP+gh5CZCn1GoqKiGDJkCOHh4bRp04Zu3bpZbN/2Qgl+Nkiv1UdEREQkp8ohs2SmER8fz+DBgzl58iT16tVj5syZaca4u7sTERFBbGz6M+ClVu7t+WFXatIUERERkRwvMTGRoUOH8s8//1C9enXmz59v1pqTqmjRogAEB6f/6VBISIjZOHukBF9EREREzBgMln09qpSUFMaMGcOOHTuoUKECCxcuNHve0J0qVqwIwJEjR9Jdf/ToUeD2dOf2Sgm+iIiIiJjJKQ+6AjAajbz11lv8+uuvlClThq+++oq8efNmOL5p06YAbNy4EaPR/F6Ca9eusW/fPpycnGjcuPEjRpZzKcEXERERkRxrxowZ/PDDDxQvXpwlS5aQP/+9H0DZrFkzSpcuzalTpwgMDDQtT0hIYOLEiSQlJfHss8/i7e2d1aFbjW6yFREREREzOaUCvHnzZhYvXgxAsWLF+Oijj9Id16JFC1q0aAGAk5MTs2fPplevXsyePZsNGzZQqlQpDh48SFBQEL6+vowZMya7TsEqlOCLiIiIiBlL9M1bwq1bt0xf7969O8NxxYoVMyX4cPsptmvWrOHTTz9l165dnDp1isKFC9O/f3+GDBli1zPogBJ8EREREcmhunTpQpcuXTK1balSpZg1a5aFI7INSvBFRERE5C45pIQvmaIEX0RERETMGJTg27Sccg+FiIiIiIhYgCr4IiIiImLGYFAN2JYpwRcRERGRu6hFx5bp8kxERERExI6ogi8iIiIiZnSTrW1TBV9ERERExI6ogi8iIiIid1EF35YpwRcRERERM5pFx7bppyciIiIiYkdUwRcRERGRu6hFx5YpwRcRERERM5pFx7apRUdERERExI6ogi8iIiIiZlTBt21K8EVERETkLmrysGX66YmIiIiI2BFV8EVERETEjMGgFh1bpgRfRERERO6iBN+WqUVHRERERMSOqIIvIiIiImY0i45tU4IvIiIiIndRk4ct009PRERERMSOqIIvIiIiImbUomPblOCLiIiIiBlNk2nb1KIjIiIiImJHVMEXERERkbuogm/LlOCLiIiIiBmDmjxsmn56IiIiIiJ2RBV8EREREbmLWnRsmRJ8ERERETGjWXRsm1p0RERERETsiCr4IiIiInIXVfBtmRJ8ERERETGjWXRsm356IiIiIiJ2RBV8EREREbmLWnRsmRJ8ERERETFjUIJv09SiIyIiIiJiR1TBFxEREREzmgfftinBFxEREZG7qMnDlumnJyIiIiJiR1TBFxEREREzusnWtinBFxEREZG7KMG3ZWrRERERERGxI6rgi4iIiIgZzaJj25Tgi4iIiMhd1ORhy/TTExERERGxI6rgi4iIiIgZzaJj2wxGo9Fo7SBERERERMQy1KIjIiIiImJHlOCLiIiIiNgRJfgiIiIiInZECb6IiIiIiB1Rgi8iIiIiYkeU4IuIiIiI2BEl+CIiIiIidkQJvoiIiIiIHVGCLyIiIiJiR5Tgi4iIiIjYESX4IiIiIiJ2RAm+iIiIiIgdUYIvIiIiImJHlOCLiIiIiNgRJfgiIiIiInZECb6IyGMiLi6Oo0ePkpycbO1QREQkCynBFxF5TIwZM4a+ffvy77//KskXqzEajWb/FRHLU4IvYkV3/oFTwiVZ7Y033qBQoUJMnDiR/fv363dOslXq75vBYDD7rxJ9EcszGPV/lki2MhqNpj9skZGRODg44O7ublomkpWuXLnCK6+8QkpKClOmTKFmzZo4OjpaOyyxc0lJSTg5ORETE8OiRYu4ceMG7u7udO7cmfLly1s7PBG7owRfxErWrVvH8uXLCQ0NxcXFhT59+tCoUSOKFCli7dDEzgUFBTFgwAAl+ZItUosa0dHRPPvss6aChtFo5OzZs4waNYq2bdtSoEABa4cqYjeU4ItYwS+//MKbb77JCy+8QJEiRbh8+TIbNmygRo0ajB49mlKlSlk7RLETd35idCcl+ZKdkpOTeeONN7hy5QozZsygcOHCuLq60qtXL/777z+++uorKlasaO0wReyGevBFsllkZCRff/01zz//PMOGDaNPnz689dZbODo6EhYWhpOTk7VDFDuRnJxs1g4WGRlJfHw8AMWKFWPhwoU4ODioJ1+yXGxsLOfOnSMgIIBSpUrh6urKL7/8wv79+3n55ZepWLEiKSkpgHryRSxBCb5INouJieHs2bNUrlwZDw8PAAYNGoTRaOStt96iWLFiBAUFER0dbeVIxZYlJyebKvIzZ85kyJAh9OjRg1dffZWQkBAgbZKv2XUkqyQkJHDr1i2cnZ2B2y2Ko0aNYvjw4QwYMICoqCimTp1KSEiI7kcSsQAl+CLZLFeuXHh4eHDz5k0ABg4cyIkTJ1i4cCEVK1bkxIkTfPLJJ1y5csXKkYqtMhqNpuR+xIgRrF+/nubNm9OnTx+OHDnCCy+8wOnTp4H/JfnOzs4MGzaMgwcPWjN0sQN3V+CNRiOenp54e3uzZ88e1qxZw+jRo3n99dd55ZVXADhx4gQHDx7k+PHj1ghZxO4owRfJIhl9zOzq6kqJEiX45ZdfePHFFzlx4gSBgYFUqFCBpKQk9u3bx5kzZ0hKSsrmiMVepFZAP//8c06ePMnHH39Mnz59iIiIIDY2lsTERPr06WOW5M+bN48SJUroRkd5JElJSRgMBpKTk4mLiyMlJQWDwYCzszMjRoxg3759jB07lpEjRzJo0CAMBgNnzpxh1qxZFCxYkMaNG1v7FETsgm6yFckCd97YeObMGa5du0aBAgXImzcvPj4+nDlzhpdeeonQ0FBmzJhB586diYiIYMuWLbz33nu8/vrr9OrVy8pnIbbmzracxMREPv/8c1xcXBgwYACLFi1i9uzZvP/+++TPn5+RI0fi5ubGwoULKVeuHPC/qQxFMiMlJQUHBweioqIYPXo0wcHBxMbG0q5dO1q3bk2FChVYsWIFn3zyCVWqVKFt27Zcu3aNzZs3YzQaWbVqFU5OTqb9iEjmKcEXyUKrV69mxowZJCQk4OTkROHChRk7diwNGjTg0KFDDBo0iHz58uHp6YmrqyunT5/mpZdeYuDAgUDGM6CI3O3O35UvvviC3r17c+bMGQoVKsSNGzcYOHAgffr0oVevXhgMBsaNG8fq1atxcXHhhx9+4IknnrDyGYgtS03KExISeOaZZ8iVKxd169blxo0b7N27Fy8vL8aPH0/t2rXZtWsXH374IdHR0Xh7e1O+fHneeustnJycdJEpYiH6v0jEgu5Mso4fP877779Pr169aNSoEefPn+eXX37hlVdeYdasWbRr144ffviBjRs3cvr0aZ544gn69u1LQEAAgKpY8sDu/F155513+O6772jYsCEVKlTAYDCwe/duYmJiqFu3run3M3/+/LRo0QJA02PKIzEajabk/tKlS5QpU4Y333yTEiVKALB582aWLl3KtGnTmDZtGvXq1WPFihVER0fj5uaGq6sroE+QRCxJ/yeJWFBq8rR//36SkpIICAigd+/e5MmTh2rVqlGzZk3mz5/PuHHjKFKkCDVq1KBPnz5p9qPkXh5G6u/KoUOHiIiIYP78+ZQrV870+5iYmIjBYODSpUv4+fkRFRXFpUuXqFGjBr169cLFxcWa4YuNMxgMJCQkMHToUE6fPo2Hhwfe3t6mgkfqheS0adPYvHkzFSpUwNnZGS8vL9M+jEajknsRC1IGIWJhwcHB9O/fn969e3P16lXTVJgAJUqUoHfv3hQvXpxff/2VlJQU09zPd1JyLw/rvffeY+rUqZw4cYJKlSrh7Oxs+t1q1qwZPj4+vP/++wwePJiRI0eya9cuWrRooeReLCI+Pp6CBQvi5OREQkICzs7OpsQfoEWLFtSoUYN169alOxWrWhFFLEtZhIiFeXl5MXnyZPz8/Lhw4QLnz58HMP1Rq1ixIsWLF+fQoUOmj7ZFHpTRaEx3hqYyZcpw9uxZzp8/z4kTJ4DbF4qJiYnkzp2bJUuW4O/vT1RUFCkpKSxfvlxPTJZMu/t30NPTk5EjR9KiRQsuXbrEmDFjAHBxcTGN9fLywsPDQzOEiWQDfR4m8gjSuwk2V65cNG3aFEdHR6ZNm8bEiRP57LPPyJMnD3C70pUrVy4MBgPx8fG4u7tbI3SxQSkpKWzdupXY2Fg6duwIwMSJE2ncuDE9e/bE09OTiRMnsnjxYry9vfH398fZ2ZnExES8vLyYOXMmjo6Opt9BkcxI7ZVPTk4mMjISo9GIi4sL3t7e9O/fH6PRyE8//cSwYcOYOXMm8fHxhIaG8s8//1CmTBlTz72IZB0l+CKZdGdyf/LkSa5evUpsbCxVq1alWLFitGzZEoPBwOTJk+nbty/9+vXDy8uLM2fOsHXrVsaNG6fkXh5KSkoKf/31F1u2bCEqKoodO3awb98+nn/+eQA6duxIfHw8s2fPJjAwkIEDB1K5cmWcnZ1JTk429TgruZfMSv09ioqKYuzYsVy6dInw8HCeeOIJhgwZQs2aNRkwYAAAS5YsoX379qZnfzg6OvLhhx8CmiFMJKtpmkyRR7R69Wo++ugj4uPjSUxMxGg0Mnr0aDp16oSbmxubNm1i5syZXLlyhUqVKlGuXDkqVqxI3759Af2hk4eTnJzMgAEDOHDgAE5OTixYsIAaNWqYzYG/atUqPvzwQ+rUqcOgQYOoVKmSlaMWexIbG8uzzz6Lp6cnTZs2JTw8nN27d3P69GlmzZpFmzZtCAsL44svvmDTpk3kyZOHWbNmUbZsWUCz5YhkB/0fJvII/vzzTyZNmsSQIUNo0KABiYmJrF69mqlTp3Lz5k0GDRpEkyZNMBqNzJs3D0dHR6ZMmYKbmxug2XLkwRmNRlP11NPTk9jYWAoUKMChQ4eoUqWKqRXH2dmZrl27AjBnzhxmz57NmDFjqFChgpXPQGxdajFi5cqVGAwG3n//fUqXLg3Af//9x/z58xk9ejSFChWiRo0a9O/fn5SUFDZt2sRXX33F1KlTAd1QK5IdlFmIZELqB19///03VapU4fnnn6dKlSrUqFGDKVOmMGjQID777DN27dqFm5sbAQEBvPrqq1y8eJFBgwaZZpZQci8PIiUlBYPBYKp69u7dm2XLllG2bFmWLVvG119/bZq5JPUGxq5duzJ48GAuXrxoNh2hSGalJubXrl0jMTGR/Pnzm9aVL1+ewYMHU65cOT7//HNiYmLw9vZm4MCBtG7dml27djFixAhAz10QyQ7KLkQyIfUP3fXr14mIiDDdQJs6LeHAgQOpWbMmCxYsIDY2Fg8PD5o3b87kyZM5ffo0zz77rGaSkAeWeiH41ltvsWjRIvz8/KhVqxbz5s2jePHiLFu2jBUrVphaH6Kjozly5Ag9e/Zk9erVFC5c2MpnILbqzml8UwsbBoOB6Oho079hiYmJADzxxBPUqVOHU6dOmdbly5ePgQMH0rBhQ/777z+uXbuWzWcg8nhSgi+SCal/9AoXLsy1a9f4999/zaa8zJUrF6VLl+bmzZumapWrqytNmzbljTfeoHbt2upBlYd2/fp1Pv74Y3755RfCw8Px8PBg7ty5lChRgq+//ppFixZx4cIFZsyYwbhx47h16xa5c+e2dthioxISEkxPqD179iwnT54EoEePHiQnJzNp0iQAnJ2dTdt4eXmRL18+0/uUlBS8vLwYOXIkixcvxsfHJ1vPQeRxpZtsRe7jzptg4+LicHZ2NiXtERERPP3005QuXZqpU6eaHs2emJjIe++9x+XLl5kzZw65cuUyJf+pfdJ371vkTnfen3HnDbRvvvkmGzZsYNy4cbRp0wYvLy+ioqIYMWIEhw4dws3NjeTkZObPn4+/v781T0Fs0KlTpwgODqZx48YAREZG0rdvX4KCgggLC6NLly707t2bXbt2MWfOHBo1asSkSZNwcHAgNDSUUaNGUa5cOdNsOaB/50SsQQm+SAZiYmJwd3c3JVcbN25k/fr1hIWF8cwzz1CzZk1Kly7Nzp07GTVqFEWLFqVHjx4UL16cEydO8PHHHzNu3Dh69Ohh7VMRG5NeQpSQkGB66uwbb7zBhg0bGD9+vCnJj42NZePGjcTHx1O/fn3TxabIg0pISGDw4MEcPXqU999/n4CAAAYMGEBSUhLt2rUjJiaGjz/+mLp16/Liiy/y33//ERgYSEpKCu7u7jg5OeHu7s6qVatwcnJSYi9iRUrwRdIRGBjI4sWLWbt2LQUKFGD9+vWMHTuWunXrEh4ezpkzZ2jZsiWvvPIKTzzxBEeOHGHChAlcvXqVuLg4ChUqRNeuXU3zQesPnWTGjBkzuHLlCnPmzAHMk/zRo0ezZcsW3nzzTVq1aoW3t7c1QxU7ce7cOaZMmUJQUBCjRo3ir7/+4plnnqFGjRoAHDp0iL59+1KjRg2GDBlCgQIFWL16NQkJCRQqVIiePXvi6OioqTBFrEwJvshdjEYj27ZtY9q0abi5ubF8+XKWLFmCk5MTL730Eh4eHixevJilS5fi7+/P4MGDqVChAsnJyRw9epSUlBQ8PT0pV64coKkwJXNiYmKYPn06f/zxB40bN2bKlCnA/5L8iIgIXnrpJSIjI+nduzfPPvus+u3FIi5evMhbb71FUFAQCQkJrF27Fm9vb1N74ZEjR+jTpw+VKlVi0qRJpvntU93ZUiYi1qGsQ+QuBoOBxo0bM3nyZGJjY3n++ef5559/KFeuHB4eHgD06dOHvn37cvjwYT777DOOHTuGo6MjVatWpXr16qbk/s4bb0Xu5c7ZSgDc3d0ZMWIEbdu25ffff+ett94CMFXwPT09KVy4MBERESxcuDDN9iKZVbJkSd59911KlCjB9evX2bVrF4BpGtYqVaqwZMkSTp06xdChQzlz5gzwv1l2lNyLWJ8yD5F0ODk58eSTT/LOO+/g4eHB3r17SU5OBjDNYf/iiy/St29fjh07xrx58zh69Gia/agtRx5EcnKy6ULwxo0bREZGEhUVhbe3N6+88gpt27bljz/+YPz48aZtbty4QZ48eVizZg0//fSTaapWEUsoVaoU7777LtWrV+f9999n+/btwO1/G5OSkqhcuTILFiygcOHCpodd6d87kZxDLToi95CYmMiuXbuYOXMmCQkJLF++nIIFC5rNhLNkyRI+/fRTZs+eTUBAgJUjFltzZwvX5MmTOXz4MDdv3qRw4cKmJyRHREQQGBjI2rVrKVOmDM2bN2fv3r0cPHiQVatWUahQISufhdirixcvMmHCBEJCQhg/fjxNmjQBSNNjr7YckZxFCb7IfSQlJfH3338zZcoUXFxcWLJkCfnz5zdL8k+cOEGFChWsHKnYslGjRrF3715efvllYmNjuXTpEj/++CPjx4+nd+/eREZGsm7dOlauXMnNmzcpWLAg06ZN0++dZLnUJP/atWuMHz9ehQwRG6AEX+QBPEiSD7qhVjJn9+7dTJw4kQkTJtCgQQMcHR05duyYac7xUaNG4erqSkpKCklJSYSEhODl5YWnp6e1Q5fHxMWLF5k4cSKHDh3iiy++oGbNmtYOSUTuQZmIyANwcnLiqaeeYuLEiSQnJ9OzZ0+uX79ultwDSu4lU27cuEFYWBjFihXD0dGRCxcu0KdPH9q3b8+IESNwdXUlODgYBwcHXFxcKFGihJJ7yVYlS5Zk4sSJPPPMM1SrVs3a4YjIfSgbEXlAqUn+uHHjiIiIMM0sIfIw7pztJvXr2NhYDAYDpUuX5sqVK3Tt2pUGDRowZcoU3Nzc+PHHH3nvvfeIjIy0VtgilC1blrfffhtHR0fTpAMikjOpRUfkISUlJXHt2jWKFi1q7VDExtx5I2JoaCiJiYkUKVKEiIgI2rdvzxNPPMGxY8do2LAhkydPJnfu3ISEhDB79mwA06xOIiIi96IEXx47j/JU2bu3Vc+9ZMb48ePZs2cPoaGhdOrUiZdffpk///yTr776iri4ODZv3oybmxuXL19m3rx5/PnnnyxZsiTNA4VERETSowRfHls7duzg4MGDREVF8eKLL5r6n+/nziT/US4W5PFxZ+V++vTpbN68mWeeecY09WrdunXp2bMnp0+fJjAwkPz58+Pl5UVKSgqXL19m4cKFVKxY0cpnISIitkIJvjyW1qxZwzvvvEORIkUICwvDwcGBsWPH0qJFC3Lnzp3hdncm9Bs3bsTd3Z2GDRsqyZcHcuLECTZu3EilSpVo2bIlAAcOHKB///7UrFmTgQMHkj9/flavXk1UVBTlypWjcePGFC9e3MqRi4iILVGCL4+dhIQE3n77bSpXrkzbtm1xcnLi3XffZcuWLYwdO5aOHTumm+TfmdwvXbqUadOm8dlnn9G0adPsPgWxQYGBgXz22We4urqycOFC/P39TdOsHj58mJdffpmKFSsyadIkypUrZ+1wRUTEhql5WB4r27dvZ+TIkVy4cIGqVatSsGBB8uXLx4cffkjr1q2ZPn0669atIyoqymy7O5P7ZcuW8f777/Puu+8quZcH1qxZM6pXr05ERARnzpwBwNnZmaSkJPz9/Vm8eDH//fcfI0aM4Pjx48Dt3zsREZGHpQRfHhuJiYn8+eefHD58mMuXL1OmTBngdkUf4IMPPqBNmzZ88MEHrF271pTk353cT5s2jUmTJtG1a1frnIjYpHLlyjF58mSqVq3KrFmz2L59O3B7+tWkpCSqVKnCggULiIqKIk+ePABq/RIRkUxRi448VsLCwvjqq6/46quvaNGiBXPmzAFuJ/kuLi4AjBw5kvXr1/PTTz/h6+tr2nbJkiXMnDmTd955R8m9ZNqlS5cYP348165dY9y4cTRp0gS4Pf2qk5OT2e+iiIhIZijBF7uU3uw2qTOZhIWFERgYyLp16wgICGDq1KmAeZL/559/0rBhQ9O2mzdv5rXXXmPKlCl069Yt+05E7NLFixeZMGECISEhTJgwgYCAAGuHJCIidkQJvtidO5P7HTt28Pfff3Px4kWeeOIJ2rRpQ4UKFUxJ/q+//krDhg3TTfLhfxcFO3bswGAw0LhxY6uck9ifixcv8s4773Ds2DFmz55tdkEpIiLyKJTgi91avXo177zzDtWqVSM6OprQ0FBu3brFzJkzadmypSnJ37RpE3Xq1GHGjBnWDlkeM+fOneP9999n3LhxlCpVytrhiIiInVCCL3bp9OnTDBw4kB49etCjRw/y5MnDgQMHWLx4MZs3byYwMJB69eoRFhbGZ599xg8//MC8efOoV6+etUOXx0zqVJkiIiKWogRf7NI///zDkCFDmD9/PnXr1jUtv3DhApMmTeLKlSssW7YMHx8fwsLCOHv2LLVr17ZixCIiIiKWoWkyxa6kXq/euHGD+Ph4Uz996lSYpUqVolOnToSGhnLt2jUAvL29Tcl9SkqKFaIWERERsRwl+GLT7vwAKi4ujtjYWAAaNWpEoUKFmDt3LgAuLi4kJycDUL58eZKTk4mJiUmzPwcH/S8hIiIitk3ZjNi01Nly1q9fz4ABA+jevTuzZ88mKiqKgQMHsm/fPkaNGgWAo6MjiYmJ7N27Fy8vL7y9va0ZuoiIiEiWUA++2KQ7p8LcvHkzI0eOpEmTJhgMBnbs2EH9+vV54YUXOHr0KIGBgRQqVAh/f38Afv31V4YMGcKAAQOseQoiIiIiWUIJvti0GzdusGPHDq5cucKgQYNwcnJi//79DBgwgJo1a9KvXz8cHR1ZtGgRFy5coFixYrRo0cL0JNr0HoglIiIiYsuU4IvN2r17N2+++SYODg4MGjSIbt26kZKSgoODA4cPH+bll1+mSpUqvP3225QrVw4wf5BV6lgRERERe6LsRmxWoUKFKFasGDdv3jTNkmMwGEhKSsLf35/Fixdz4sQJxo0bx6FDhwBwcnICblfuldyLiIiIPVKGIzardOnSzJgxA19fX+bMmcO2bdswGAw4OTmRlJRElSpVCAwM5NChQwQHBwP/myVHbTkiIiJir9SiIzbv0qVLjB8/nmvXrjFu3DiaNGkCQFJSEk5OToSFhWnGHBEREXlsKMEXu3Dx4kUmTJhASEgIEyZMICAgADC/iVY99yIiIvI4ULYjdqFkyZK89957FC1alEmTJvHbb78B5q04Su5FRETkcaCMR+xGyZIlmTx5Mvny5SMqKsra4YiIiIhYhVp0xO5ERkbi6elp7TBERERErEIJvtgtPcRKREREHkdq0RG7peReREREHkdK8EVERERE7IgSfBERERERO6IEX0RERETEjijBFxERERGxI0rwRURERETsiBJ8ERELaNasGWPHjjW93717N35+fuzevduKUZm7O8bs0KtXLzp06GDRfVrjPEREbIkSfBGxeT/++CN+fn6ml7+/P61bt2bKlCmEhoZaO7yHsmPHDj799FOrxuDn58eUKVOsGoOIiGSek7UDEBGxlGHDhlG8eHESEhLYt28fK1euZMeOHaxbtw43N7dsjaVOnTocOnQIZ2fnh9pux44dfP311wwdOjSLIhMREXunBF9E7Ebjxo3x9/cHoGvXrnh5ebFo0SK2bNmSYZtITEwM7u7uFo/FwcEBV1dXi+9XRETkftSiIyJ266mnngLg8uXLAIwdO5YaNWpw8eJFXnnlFWrUqMHo0aMBSElJYfHixbRv3x5/f3/q16/PxIkTiYiIMNun0Whk/vz5NG7cmGrVqtGrVy9Onz6d5tgZ9eAfPHiQV155hTp16lC9enU6duzIkiVLTPF9/fXXAGYtR6ksHeOj2Lx5MwMGDKBhw4ZUqVKFFi1aMG/ePJKTk9Mdf+TIEXr06EHVqlVp1qwZK1euTDMmISGBOXPm0LJlS6pUqUJAQAAffPABCQkJFo1dRMTeqYIvInbr4sWLAHh5eZmWJSUl0a9fP2rVqsWbb75Jrly5AJg4cSKrV6+mS5cu9OrVi8uXL/P1119z7NgxVq5caWq1+eSTT/jss88ICAggICCAo0eP0rdvXxITE+8bz19//cXAgQPx8fGhd+/eFChQgDNnzrB9+3ZeeuklunfvzrVr1/jrr7/44IMP0myfHTE+qNWrV+Pu7s7LL7+Mu7s7f//9N3PmzCEqKoo333zTbGxERAQDBgygbdu2tG/fnl9//ZVJkybh7OzMc889B9y+eBk8eDD79u2jW7dulCtXjlOnTrFkyRLOnz/P/PnzLRa7iIi9U4IvInYjKiqKsLAwEhIS2L9/P/PmzSNXrlw0bdrUNCYhIYE2bdowatQo07K9e/eyatUqZs2aRceOHU3Ln3zySfr378+GDRvo2LEjYWFhfPHFFzRp0oQFCxZgMBgA+Oijj1iwYME9Y0tOTmbixIn4+PiwZs0a8uTJY1pnNBoBqFGjBqVLl+avv/6iU6dOZttnR4wPY/bs2aaLI4Dnn3+eiRMnsnLlSkaMGIGLi4tp3bVr1xg7diwvv/wyAN27d6dbt258+OGHdOrUCWdnZ37++Wd27tzJsmXLqF27tmnbJ554gnfeeYf9+/dTs2ZNi8UvImLP1KIjInajT58+1KtXj4CAAEaMGIGHhwdz586lUKFCZuOef/55s/cbNmzA09OTBg0aEBYWZnpVrlwZd3d3U5vNzp07SUxM5MUXXzQlzgAvvfTSfWM7duwYly9fpnfv3mbJPWC2r4xkR4wP487kPvXCqnbt2sTGxnL27FmzsU5OTnTv3t303sXFhe7du3Pjxg2OHj1qOr9y5cpRtmxZs/NLbbPKSdONiojkdKrgi4jdmDhxImXKlMHR0ZECBQpQpkwZHBzM6xhOTk4ULlzYbNmFCxeIjIykXr166e73xo0bAFy5cgWA0qVLm6339vYmb96894zt0qVLAPj6+j7w+WR3jA/j9OnTfPzxx/z9999ERUWZrYuMjDR77+Pjk+ZG5tT4goKCqF69OhcuXODMmTP3PT8REbk/JfgiYjeqVq1qmkUnIy4uLmmS/pSUFPLnz8+sWbPS3cbb29tiMWZWTorx1q1bvPjii+TOnZthw4ZRsmRJXF1dOXr0KLNmzSIlJeWh95mSkoKvry/jxo1Ld/3dF2UiIpIxJfgi8tgrWbIku3btombNmmatJ3crWrQoAOfPn6dEiRKm5WFhYWlmsrlb6vhTp05Rv379DMdl1K6THTE+qH/++Yfw8HDmzp1LnTp1TMtTZyu627Vr19JMR3r+/HkAihUrBtw+vxMnTlCvXr0HalkSEZGMqQdfRB57bdu2JTk5Od2ZWpKSkrh16xYA9evXx9nZmeXLl5tujAVM01zeS+XKlSlevDhLly417S/VnftKfSDX3WOyI8YHlfoJyJ37T0hIYMWKFemOT0pK4ttvvzUb++233+Lt7U3lypWB2+cXEhLCd999l2b7uLg4YmJiLBa/iIi9UwVfRB57devWpXv37nz++eccP36cBg0a4OzszPnz59mwYQMTJkygTZs2eHt707dvXz7//HP+r507dkkmjuM4/vEOhJYksJYUbJfAECFI2lyEMxrabmo4nHJ00kiHnGoRBE10EzcFNWhzcxQkl3Drz4hreHgk6Xmenil4fs/7NR53/L63ve/48fM8T6enp3p+ftZ0OtXOzs4f17AsS9fX18rn8zo7O9P5+bl2d3e1Wq308vKih4cHSVoHb7Va1cnJiWzbVjab/ZYZP1osFr/8mEilUkokEgqFQioWi3JdV4FAQIPBYCP4P9rb21Oz2dTr66tisZjG47GWy6Uqlcr6aM9cLqfJZKJyuazZbKajoyO9vb1ptVrp8fFRrVbry+1XAIAfCHwAkHRzc6N4PK5er6e7uzvZtq39/X05jrNxPGOhUFAwGFSv19NsNtPh4aHa7bY8z/tyjXQ6rW63q3q9rna7Ld/3FY1GdXFxsb4nk8nIdV2NRiMNh0P5vq9sNvttM/40n881n88/Xb+6ulIymVSj0VCtVtP9/b22t7flOI6Oj491eXn56ZlQKKTb21tVq1X1+32Fw2GVSqWN97YsS/V6XZ1OR4PBQE9PT9ra2lIkEpHrujo4OPjr2QHgfxfwf/fLBQAAAMA/hz34AAAAgEEIfAAAAMAgBD4AAABgEAIfAAAAMAiBDwAAABiEwAcAAAAMQuADAAAABiHwAQAAAIMQ+AAAAIBBCHwAAADAIAQ+AAAAYBACHwAAADAIgQ8AAAAY5B00mnbs3k1YNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dfMAJ=df\n",
    "\n",
    "dfMAJ['labelAsma'] = dfMAJ['labelAsma'].replace({'hope': 1, 'non_hope': 0, 'neutral':2})\n",
    "\n",
    "# Remplacer les valeurs dans la colonne 'labelGPT'\n",
    "dfMAJ['labelGPT'] = dfMAJ['labelGPT'].replace({'hope': 1, 'non_hope': 0, 'neutral':2})\n",
    "dfMAJ1=dfMAJ\n",
    "dfMAJ1 = dfMAJ1.dropna(subset=['labelGPT'])\n",
    "print(dfMAJ1['labelGPT'].isna().sum())\n",
    "\n",
    "\n",
    "true_labels = dfMAJ1['labelAsma']\n",
    "predicted_labels = dfMAJ1['labelGPT']\n",
    "\n",
    "unique_values = dfMAJ1['labelAsma'].unique()\n",
    "\n",
    "# Afficher les valeurs uniques\n",
    "print(unique_values)\n",
    "\n",
    "true_labels = true_labels.astype(float)\n",
    "predicted_labels = predicted_labels.astype(float)\n",
    "\n",
    "#confusion_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "MyshowAllScores(true_labels,predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khNnr0e_pRa3"
   },
   "source": [
    "# Netoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrQkQxv8sR1b",
    "outputId": "91338d0e-df26-49de-aa81-d6c487c91e96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_uri(text):\n",
    "  # expression régulière pour détecter les URI\n",
    "  pattern = r'https?://\\S+|www.\\S+'\n",
    "  # suppression des URI\n",
    "  return re.sub(pattern, ' ', text)\n",
    "\n",
    "def removeHashtags(text):\n",
    "    cleanedText = re.sub(r'#\\w+\\s*', '', text)\n",
    "    cleanedText = cleanedText.strip()\n",
    "    return cleanedText\n",
    "\n",
    "def replace_emojies_meaning(text):\n",
    "    emoji_df = pd.read_excel('frenchEmojies.xlsx')\n",
    "    emoji_dict = dict(zip(emoji_df['EMOJI'], emoji_df['SIGNIFICATION']))\n",
    "    words = text.split()\n",
    "    meanings = []\n",
    "    for i, word in enumerate(words):\n",
    "        word_meanings = []\n",
    "        for emoji, meaning in emoji_dict.items():\n",
    "            if emoji in word:\n",
    "                word_meanings.append(meaning)\n",
    "                word = re.sub(re.escape(emoji), \"\", word)\n",
    "        if word_meanings:\n",
    "            meanings.extend(word_meanings)\n",
    "            words[i] = word\n",
    "    words.append(\" \".join(meanings))\n",
    "    return \" \".join(words)\n",
    "\n",
    "def eliminer_abbreviation(text):\n",
    "  dicAbreviation = {\n",
    "      \"toujours\" : ['tjrs','tjr'],\n",
    "      \"merci\": ['mrc','cimer', ],\n",
    "      \"pour\": ['pr'],\n",
    "      \"donc\": ['dc','dnc'],\n",
    "      \"désolé\": ['dsl'],\n",
    "      \"mais\": ['ms'],\n",
    "      \"quelqu'un\": ['qqn'],\n",
    "      \"quelque chose\": ['qqch'],\n",
    "      \"s'il vous plaît\": ['svp'],\n",
    "      \"beaucoup\": ['bcp'],\n",
    "      \"salut\": ['slt'],\n",
    "      \"jamais\": ['jms'],\n",
    "      \"quelque\": ['qlq'],\n",
    "      \"quand\": ['qd'],\n",
    "      \"quand même\": ['qmm'],\n",
    "      \"pourquoi\": ['pk','prq','pq'],\n",
    "      \"comment\": ['cm'],\n",
    "      \"maintenant\": ['mtnt','mnt','mtn'],\n",
    "      \"tout\": ['tt'],\n",
    "      \"tous\": ['ts'],\n",
    "      \"pour\": ['pr'],\n",
    "      \"s'il te plaît\": ['stp'],\n",
    "      \"dans\": ['ds','dns'],\n",
    "      \"nouvelle\": ['nvl'],\n",
    "      \"nouveau\":['nv'],\n",
    "      \"bien\":['b1']\n",
    "  }\n",
    "  for mot, abbreviations in dicAbreviation.items():\n",
    "      for abbreviation in abbreviations:\n",
    "          pattern = r\"(?<!\\w)\" + re.escape(abbreviation) + r\"(?!\\w)\"\n",
    "          text = re.sub(pattern, mot, text)\n",
    "  return text\n",
    "\n",
    "def replace_mentions(text):\n",
    "    cleaned_text = re.sub(r'@\\w+', 'UTILISATEUR', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean(removeUri, removeHashtag, replaceMentions, replaceEmojies, eliminerAbbreviation,lowerCase, text):\n",
    "  if lowerCase:\n",
    "    text=text.lower()\n",
    "  text=text.replace(\"&amp;\",\"\")\n",
    "  if replaceEmojies :\n",
    "    text = replace_emojies_meaning(text)\n",
    "  if removeUri :\n",
    "    text = remove_uri(text)\n",
    "  if removeHashtag :\n",
    "    text = removeHashtags(text)\n",
    "  if replaceMentions :\n",
    "    text = replace_mentions(text)\n",
    "  if eliminerAbbreviation:\n",
    "    text = eliminer_abbreviation(text)\n",
    "  return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azaqkA04iPCk"
   },
   "source": [
    "# BERT multilingual base model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAMzJYW1SamM"
   },
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNvB6cqHxtpy",
    "outputId": "989c7485-e4c6-4030-92f3-c28bb937173c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y786vy1l7VYq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.functional import sigmoid\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HjjALEhv00X"
   },
   "source": [
    "##Focal Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBXa0Xr5Y4pL"
   },
   "source": [
    "###TRAIN (40/60) best score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmyheRfZY4pN",
    "outputId": "cebc7ee6-dacb-46f5-d1a4-f3a55fc53b6b",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dfMAJ['tweet'] = dfMAJ['tweet'].apply(lambda x: clean(1,0,1,1,1,1,x))\n",
    "dfMAJ = dfMAJ[dfMAJ['labelAsma'].isin([1, 0])]\n",
    "label_values = dfMAJ['labelAsma'].unique()\n",
    "\n",
    "# Afficher les valeurs de label\n",
    "print(label_values)\n",
    "print(len(dfMAJ))\n",
    "\n",
    "X=dfMAJ['tweet'].tolist()\n",
    "y=dfMAJ['labelAsma'].tolist()\n",
    "texts, test_texts, labels, test_labels = train_test_split(X, y,train_size=0.6,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEJBjKFKaC9Z",
    "outputId": "0e2371a6-9754-4db2-cf82-f7d866726f33",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-xt8vrvphbN",
    "outputId": "e44eb902-d868-4a18-9bb8-8a1f63e9ef58"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Results:\n",
      "[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
      "[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]\n",
      "[1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]\n",
      "[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]\n",
      "[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]\n",
      "[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]\n",
      "[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]\n",
      "[0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]\n",
      "[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]\n",
      "[1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-hope       0.84      0.78      0.81        65\n",
      "        Hope       0.77      0.82      0.79        56\n",
      "\n",
      "    accuracy                           0.80       121\n",
      "   macro avg       0.80      0.80      0.80       121\n",
      "weighted avg       0.80      0.80      0.80       121\n",
      "\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "[1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1.]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        sample = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.float).unsqueeze(0)  # Add unsqueeze(0) to make it [1, 1]\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Define your BERT-based classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "max_length = 128\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 8    #16,32\n",
    "num_epochs = 8\n",
    "\n",
    "# Create the train dataloader\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of the classifier\n",
    "model = BertClassifier(num_classes=1)\n",
    "\n",
    "\n",
    "# Define the focal loss function\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        probs = sigmoid(logits)\n",
    "        pt = probs * labels + (1 - probs) * (1 - labels)\n",
    "        alpha_factor = self.alpha * labels + (1 - self.alpha) * (1 - labels)\n",
    "        focal_weight = alpha_factor * torch.pow(1 - pt, self.gamma)\n",
    "\n",
    "        loss = focal_weight * nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = FocalLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print('Test Results:')\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "all_texts = []  # Add a list to store the texts\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    list=[]\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    probabilities = sigmoid(logits)\n",
    "    predictions = torch.round(probabilities).squeeze()\n",
    "    p = predictions.tolist()\n",
    "    print(p)\n",
    "    if isinstance(p, float):\n",
    "      list.append(p)\n",
    "      all_predictions.extend(list)\n",
    "    else :\n",
    "      all_predictions.extend(p)  #Ajouter la liste de prédictions\n",
    "    all_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "target_names = ['Non-hope', 'Hope']\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(all_labels, all_predictions, target_names=target_names))\n",
    "\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "for i in all_predictions:\n",
    "    print(i)\n",
    "\n",
    "print(all_predictions)\n",
    "print(all_labels)\n",
    "data = [test_texts, all_labels,  all_predictions]\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('classification_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwOOZP4pmrQf",
    "outputId": "7f7370b1-ce9b-4894-98db-4c425b07ffdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-1a91743bd283>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfMAJ['tweet'] = dfMAJ['tweet'].apply(lambda x: clean(0,0,0,0,1,1,x))\n"
     ]
    }
   ],
   "source": [
    "dfMAJ['tweet'] = dfMAJ['tweet'].apply(lambda x: clean(0,0,0,0,1,1,x))\n",
    "#clean(removeUri, removeHashtag, replaceMentions, replaceEmojies, eliminerAbbreviation,lowerCase, text):\n",
    "dfMAJ = dfMAJ.drop(dfMAJ[dfMAJ['labelAsma'] == 2].index)\n",
    "label_values = dfMAJ['labelAsma'].unique()\n",
    "\n",
    "# Afficher les valeurs de label\n",
    "print(label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlaeYYfgmrQg"
   },
   "outputs": [],
   "source": [
    "X=dfMAJ['tweet'].tolist()\n",
    "y=dfMAJ['labelAsma'].tolist()\n",
    "texts, test_texts, labels, test_labels = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6b1d3d1bf48e4686a6b4678c58637320",
      "ab6ca110c9134f5e9d4279083a190c9c",
      "84b3812bd0cb4c0bb29fd47dbdc2508f",
      "2d6df3d9207d463dabb716cfc0688392"
     ]
    },
    "id": "RcgWtK_tmrQg",
    "outputId": "8302e31b-c645-4bf6-fc11-266c688630d2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1d3d1bf48e4686a6b4678c58637320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6ca110c9134f5e9d4279083a190c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b3812bd0cb4c0bb29fd47dbdc2508f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6df3d9207d463dabb716cfc0688392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 - Loss: 0.0738\n",
      "Epoch 2/8 - Loss: 0.0563\n",
      "Epoch 3/8 - Loss: 0.0274\n",
      "Epoch 4/8 - Loss: 0.0088\n",
      "Epoch 5/8 - Loss: 0.0007\n",
      "Epoch 6/8 - Loss: 0.0001\n",
      "Epoch 7/8 - Loss: 0.0001\n",
      "Epoch 8/8 - Loss: 0.0000\n",
      "Test Results:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-hope       0.69      0.92      0.79        24\n",
      "        Hope       0.87      0.57      0.68        23\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.78      0.74      0.73        47\n",
      "weighted avg       0.78      0.74      0.74        47\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        sample = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.float).unsqueeze(0)  # Add unsqueeze(0) to make it [1, 1]\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Define your BERT-based classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "max_length = 128\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 8    #16,32\n",
    "num_epochs = 8\n",
    "\n",
    "# Create the train dataloader\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of the classifier\n",
    "model = BertClassifier(num_classes=1)\n",
    "\n",
    "\n",
    "# Define the focal loss function\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        probs = sigmoid(logits)\n",
    "        pt = probs * labels + (1 - probs) * (1 - labels)\n",
    "        alpha_factor = self.alpha * labels + (1 - self.alpha) * (1 - labels)\n",
    "        focal_weight = alpha_factor * torch.pow(1 - pt, self.gamma)\n",
    "\n",
    "        loss = focal_weight * nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = FocalLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Test Results:')\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "all_texts = []  # Add a list to store the texts\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    probabilities = sigmoid(logits)\n",
    "    predictions = torch.round(probabilities).squeeze()\n",
    "    #print(predictions.tolist())\n",
    "    all_predictions.extend(predictions.tolist())\n",
    "    all_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "\n",
    "target_names = ['Non-hope', 'Hope']\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(all_labels, all_predictions, target_names=target_names))\n",
    "\n",
    "# data = {'Text': all_texts, 'True Label': all_labels, 'Predicted Label': all_predictions}\n",
    "# print(data)\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv('classification_results.csv', index=False)\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "for i in all_predictions:\n",
    "    print(i)\n",
    "\n",
    "print(all_predictions)\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yzuq_rHc8BW"
   },
   "source": [
    "### TRAIN_less_data (BEST SCORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBIOn0JAc8Bf",
    "outputId": "b57e3d42-2eab-4412-abbc-cfa980ad9160",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dfMAJ['tweet'] = dfMAJ['tweet'].apply(lambda x: clean(0,0,0,0,0,1,x))\n",
    "#clean(removeUri, removeHashtag, replaceMentions, replaceEmojies, eliminerAbbreviation,lowerCase, text):\n",
    "dfMAJ = dfMAJ.drop(dfMAJ[dfMAJ['labelAsma'] == 2].index)\n",
    "label_values = dfMAJ['labelAsma'].unique()\n",
    "\n",
    "# Afficher les valeurs de label\n",
    "print(label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxKjilYFc8Bg",
    "outputId": "a2a42190-a058-4132-81a9-071baa2ccb0d",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X=dfMAJ['tweet'].tolist()\n",
    "y=dfMAJ['labelAsma'].tolist()\n",
    "texts, test_texts, labels, test_labels = train_test_split(X, y, stratify=y)\n",
    "print(len(texts))\n",
    "print(len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrHFgt5yc8Bg",
    "outputId": "9abd005c-008a-4141-d7e4-ebd429895822"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 - Loss: 0.0735\n",
      "Epoch 2/8 - Loss: 0.0585\n",
      "Epoch 3/8 - Loss: 0.0374\n",
      "Epoch 4/8 - Loss: 0.0187\n",
      "Epoch 5/8 - Loss: 0.0082\n",
      "Epoch 6/8 - Loss: 0.0052\n",
      "Epoch 7/8 - Loss: 0.0036\n",
      "Epoch 8/8 - Loss: 0.0055\n",
      "Test Results:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-hope       0.86      0.79      0.83        24\n",
      "        Hope       0.80      0.87      0.83        23\n",
      "\n",
      "    accuracy                           0.83        47\n",
      "   macro avg       0.83      0.83      0.83        47\n",
      "weighted avg       0.83      0.83      0.83        47\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "[0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1.]\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        sample = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.float).unsqueeze(0)  # Add unsqueeze(0) to make it [1, 1]\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Define your BERT-based classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "max_length = 128\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 8    #16,32\n",
    "num_epochs = 8\n",
    "\n",
    "# Create the train dataloader\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of the classifier\n",
    "model = BertClassifier(num_classes=1)\n",
    "\n",
    "\n",
    "# Define the focal loss function\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        probs = sigmoid(logits)\n",
    "        pt = probs * labels + (1 - probs) * (1 - labels)\n",
    "        alpha_factor = self.alpha * labels + (1 - self.alpha) * (1 - labels)\n",
    "        focal_weight = alpha_factor * torch.pow(1 - pt, self.gamma)\n",
    "\n",
    "        loss = focal_weight * nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = FocalLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Test Results:')\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "all_texts = []  # Add a list to store the texts\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    probabilities = sigmoid(logits)\n",
    "    predictions = torch.round(probabilities).squeeze()\n",
    "    #print(predictions.tolist())\n",
    "    all_predictions.extend(predictions.tolist())\n",
    "    all_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "\n",
    "target_names = ['Non-hope', 'Hope']\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(all_labels, all_predictions, target_names=target_names))\n",
    "\n",
    "# data = {'Text': all_texts, 'True Label': all_labels, 'Predicted Label': all_predictions}\n",
    "# print(data)\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv('classification_results.csv', index=False)\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "for i in all_predictions:\n",
    "    print(i)\n",
    "\n",
    "print(all_predictions)\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPTDQbEAFO2M"
   },
   "source": [
    "## BCEWithLogitsLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k49DXMl1GZUE"
   },
   "source": [
    "### Train 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGFaPT7YGlWK"
   },
   "outputs": [],
   "source": [
    "X=dfMAJ['tweet'].tolist()\n",
    "y=dfMAJ['labelAsma'].tolist()\n",
    "texts, test_texts, labels, test_labels = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UkuFRQiGrc4",
    "outputId": "a1044d44-a3e4-434c-ce31-baf68c10d22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-966d46ba9980>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfMAJ['tweet'] = dfMAJ['tweet'].apply(lambda x: clean(1,1,1,1,1,1,x))\n"
     ]
    }
   ],
   "source": [
    "dfMAJ['tweet'] = dfMAJ['tweet'].apply(lambda x: clean(1,1,1,1,1,1,x))\n",
    "dfMAJ = dfMAJ.drop(dfMAJ[dfMAJ['labelAsma'] == 2].index)\n",
    "label_values = dfMAJ['labelAsma'].unique()\n",
    "\n",
    "# Afficher les valeurs de label\n",
    "print(label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5oDaTpJAFGZA",
    "outputId": "e7d6eb1e-ad45-4a4b-adbd-d6b213d05492"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1bc0f9cfb649619927c24d6d21b487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a97bebf4f44176a34fe4f4e8894d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3469c963bc044c09064a75f4b3b1969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4093da7ad95a49209a29cf03849ec6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.6625\n",
      "Epoch 2/10 - Loss: 0.6418\n",
      "Epoch 3/10 - Loss: 0.4607\n",
      "Epoch 4/10 - Loss: 0.3539\n",
      "Epoch 5/10 - Loss: 0.1234\n",
      "Epoch 6/10 - Loss: 0.0306\n",
      "Epoch 7/10 - Loss: 0.0122\n",
      "Epoch 8/10 - Loss: 0.0075\n",
      "Epoch 9/10 - Loss: 0.0053\n",
      "Epoch 10/10 - Loss: 0.0043\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Hope       0.84      0.67      0.74        24\n",
      "        Hope       0.71      0.87      0.78        23\n",
      "\n",
      "    accuracy                           0.77        47\n",
      "   macro avg       0.78      0.77      0.76        47\n",
      "weighted avg       0.78      0.77      0.76        47\n",
      "\n",
      "Confusion Matrix:\n",
      "[[16  8]\n",
      " [ 3 20]]\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        sample = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Define your BERT-based classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define the tokenizer and maximum sequence length\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "max_length = 128\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 8  #18\n",
    "num_epochs = 10     #5,7\n",
    "\n",
    "# Create the train dataloader\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of the classifier\n",
    "model = BertClassifier(num_classes=1)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels'].unsqueeze(1).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "for batch in test_dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    predictions = torch.sigmoid(logits)\n",
    "\n",
    "    all_predictions.extend(predictions.round().tolist())\n",
    "    all_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "target_names = ['Non-Hope', 'Hope']\n",
    "print('Classification Report:')\n",
    "print(classification_report(all_labels, all_predictions, target_names=target_names))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "# Convert predictions and labels to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "print(all_predictions)\n",
    "print(all_labels)\n",
    "\n",
    "\n",
    "# Create a DataFrame with predictions and labels\n",
    "# predictions_data = {'Text': test_texts, 'True Label': all_labels, 'Predicted Label': all_predictions}\n",
    "# predictions_df = pd.DataFrame(predictions_data)\n",
    "#\n",
    "# # Save the DataFrame to a CSV file\n",
    "# predictions_df.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "159txOpGG8k2"
   },
   "source": [
    "### Train 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SHetf2KG8k_",
    "outputId": "a0efdca3-0606-4cf9-8ff7-3e0956dc67c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-e550329f0c44>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfMAJ['tweet'] = dfMAJ['tweet'].apply(lambda x: clean(0,0,0,0,0,1,x))\n"
     ]
    }
   ],
   "source": [
    "dfMAJ['tweet'] = dfMAJ['tweet'].apply(lambda x: clean(0,0,0,0,0,1,x))\n",
    "dfMAJ = dfMAJ.drop(dfMAJ[dfMAJ['labelAsma'] == 2].index)\n",
    "label_values = dfMAJ['labelAsma'].unique()\n",
    "\n",
    "# Afficher les valeurs de label\n",
    "print(label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zghQ6cmEG8k_"
   },
   "outputs": [],
   "source": [
    "X=dfMAJ['tweet'].tolist()\n",
    "y=dfMAJ['labelAsma'].tolist()\n",
    "texts, test_texts, labels, test_labels = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9c31de7d1ec3458986080a2b4d4115b8",
      "89d5d00383c44e65be542e1a8f6d9682",
      "ebeeb0865ce14c3ba60f7ddfd51aa74f",
      "016e3847ea7b48dd9518058edb2b4e27"
     ]
    },
    "id": "QQXqx0n8G8lA",
    "outputId": "e1fe91a3-66f5-4ecc-ccb9-d349806cb5da"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c31de7d1ec3458986080a2b4d4115b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d5d00383c44e65be542e1a8f6d9682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebeeb0865ce14c3ba60f7ddfd51aa74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016e3847ea7b48dd9518058edb2b4e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.6713\n",
      "Epoch 2/10 - Loss: 0.6198\n",
      "Epoch 3/10 - Loss: 0.4262\n",
      "Epoch 4/10 - Loss: 0.1456\n",
      "Epoch 5/10 - Loss: 0.0284\n",
      "Epoch 6/10 - Loss: 0.0130\n",
      "Epoch 7/10 - Loss: 0.0637\n",
      "Epoch 8/10 - Loss: 0.0338\n",
      "Epoch 9/10 - Loss: 0.0084\n",
      "Epoch 10/10 - Loss: 0.0044\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Hope       0.70      0.79      0.75        24\n",
      "        Hope       0.75      0.65      0.70        23\n",
      "\n",
      "    accuracy                           0.72        47\n",
      "   macro avg       0.73      0.72      0.72        47\n",
      "weighted avg       0.73      0.72      0.72        47\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  5]\n",
      " [ 8 15]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "[0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1\n",
      " 0 0 1 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        sample = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Define your BERT-based classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define the tokenizer and maximum sequence length\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "max_length = 128\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 8  #18\n",
    "num_epochs = 10     #5,7\n",
    "\n",
    "# Create the train dataloader\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of the classifier\n",
    "model = BertClassifier(num_classes=1)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels'].unsqueeze(1).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "for batch in test_dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    predictions = torch.sigmoid(logits)\n",
    "\n",
    "    all_predictions.extend(predictions.round().tolist())\n",
    "    all_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "target_names = ['Non-Hope', 'Hope']\n",
    "print('Classification Report:')\n",
    "print(classification_report(all_labels, all_predictions, target_names=target_names))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "# Convert predictions and labels to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "print(all_predictions)\n",
    "print(all_labels)\n",
    "\n",
    "\n",
    "# Create a DataFrame with predictions and labels\n",
    "# predictions_data = {'Text': test_texts, 'True Label': all_labels, 'Predicted Label': all_predictions}\n",
    "# predictions_df = pd.DataFrame(predictions_data)\n",
    "#\n",
    "# # Save the DataFrame to a CSV file\n",
    "# predictions_df.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Camembert model"
   ],
   "metadata": {
    "id": "jDGEZY-KfpkX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Roberta model"
   ],
   "metadata": {
    "id": "XrfTbgSrftBH"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ySzbtyHm4TP5",
    "s9ahOmVy4eGh",
    "mCxujl6V4tzW",
    "ZH05HYQt4xgn",
    "khNnr0e_pRa3",
    "VAMzJYW1SamM",
    "2yzuq_rHc8BW",
    "itkyHv5LmrQW",
    "sMjf4scXvOIX",
    "VLOT2Wl53ZOQ",
    "9ObFMVIk-fOQ",
    "XPTDQbEAFO2M",
    "k49DXMl1GZUE",
    "159txOpGG8k2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
